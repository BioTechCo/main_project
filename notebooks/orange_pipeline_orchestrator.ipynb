{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import pandas as pd\n",
    "from utils.config_helper import update_nested_toml, load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breast\n",
    "lung\n",
    "prostate\n",
    "stomach\n",
    "rectal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"stomach\"\n",
    "CONFIG_PATH = f\"../config/{TYPE}.toml\"\n",
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_nan(df, name):\n",
    "    print(df[pd.isna(df[name])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_file_number = config[\"init\"][\"hyper\"][\"beta_file_number\"]\n",
    "test_ratio = config[\"init\"][\"hyper\"][\"test_ratio\"]\n",
    "seed = config[\"init\"][\"hyper\"][\"splitting_seed\"]\n",
    "normal_number_0 = config[\"init\"][\"hyper\"][\"normal_number_0\"]\n",
    "if beta_file_number == 2:\n",
    "    normal_number_1 = config[\"init\"][\"hyper\"][\"normal_number_1\"]\n",
    "data_source = config[\"init\"][\"hyper\"][\"data_source\"]\n",
    "is_columns_duplicated = config[\"init\"][\"hyper\"][\"is_columns_duplicated\"]\n",
    "if data_source == \"GDC_stomach_GSE99553\":  # god forgive me\n",
    "    is_columns_duplicated_1 = config[\"init\"][\"hyper\"][\"is_columns_duplicated_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainOutPath = f\"../{TYPE}/result/{data_source}/train{int(100-test_ratio*100)}\"\n",
    "testOutPath = f\"../{TYPE}/result/{data_source}/test{int(test_ratio*100)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section. 0 Merge and Split Champ Data \n",
    "(if there are more than one normalized beta data)\n",
    "\n",
    "#### Summary\n",
    "- beta data is split into train and test\n",
    "- all beta data will the following format\n",
    "  - column 0 is the id of the sample\n",
    "  - column 1 to n is the beta value of each CpG site\n",
    "  - normal samples come first, then tumor samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(f\"../{TYPE}/champ_result/{data_source}/all_beta_normalized_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df1 = pd.read_csv(f\"../{TYPE}/champ_result/{data_source}/all_beta_normalized_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "df0\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "df1\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential feature loss\n",
    "if beta_file_number == 2:\n",
    "    feature_name_0 = df0.iloc[:, 0].tolist()\n",
    "    feature_name_1 = df1.iloc[:, 0].tolist()\n",
    "\n",
    "    feature_name = list(set(feature_name_0).intersection(feature_name_1))\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_0\", len(feature_name_0)\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_1\", len(feature_name_1)\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_intersection\", len(feature_name)\n",
    "    )\n",
    "elif beta_file_number == 1:\n",
    "    feature_name = df0.iloc[:, 0].tolist()\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_0\", len(feature_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df0_join = df0[df0.iloc[:, 0].isin(feature_name)]\n",
    "    df1_join = df1[df1.iloc[:, 0].isin(feature_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df0_join = df0_join.iloc[:, 1::is_columns_duplicated]\n",
    "    if data_source == \"GDC_stomach_GSE99553\":  # god forgive me\n",
    "        df1_join = df1_join.iloc[:, 1::is_columns_duplicated_1]\n",
    "    df0_join.reset_index(drop=True, inplace=True)\n",
    "    df1_join.reset_index(drop=True, inplace=True)\n",
    "    df0_join_normal = df0_join.iloc[:, :normal_number_0]\n",
    "    df0_join_tumor = df0_join.iloc[:, normal_number_0:]\n",
    "    df1_join_normal = df1_join.iloc[:, :normal_number_1]\n",
    "    df1_join_tumor = df1_join.iloc[:, normal_number_1:]\n",
    "elif beta_file_number == 1:\n",
    "    df0_join = df0.iloc[:, 1::is_columns_duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df_normal = pd.concat([df0_join_normal, df1_join_normal], axis=1)\n",
    "    df_tumor = pd.concat([df0_join_tumor, df1_join_tumor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those samples with missing value\n",
    "# note: could use padding or other methods to fill the missing value\n",
    "\n",
    "if beta_file_number == 2:\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"Before_dropna_dfn_shape\", df_normal.shape\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"Before_dropna_dfc_shape\", df_tumor.shape\n",
    "    )\n",
    "    df_normal.dropna(inplace=True, axis=1)\n",
    "    df_tumor.dropna(inplace=True, axis=1)\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"After_dropna_dfn_shape\", df_normal.shape\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"After_dropna_dfc_shape\", df_tumor.shape\n",
    "    )\n",
    "elif beta_file_number == 1:\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"Before_dropna_df_shape\", df0_join.shape\n",
    "    )\n",
    "    df0_join.dropna(inplace=True, axis=1)\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"After_dropna_df_shape\", df0_join.shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df = pd.concat([df_normal, df_tumor], axis=1)\n",
    "    df.columns = range(df.shape[1])\n",
    "elif beta_file_number == 1:\n",
    "    df = df0_join\n",
    "    df.columns = range(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "df\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "if beta_file_number == 1:\n",
    "    normal_count = config[\"init\"][\"hyper\"][\"normal_number_0\"]\n",
    "elif beta_file_number == 2:\n",
    "    normal_count = config[\"init\"][\"hyper\"][\"normal_number_0\"] + config[\"init\"][\"hyper\"][\"normal_number_1\"]\n",
    "X = df.T\n",
    "y = [(0 if i < normal_count else 1) for i in range((df.shape[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=seed\n",
    ")\n",
    "\n",
    "train_class_distribution = Counter(y_train)\n",
    "testing_class_distribution = Counter(y_test)\n",
    "\n",
    "update_nested_toml(\"preprocess.merge_and_split\", \"training_set_samples\", len(X_train))\n",
    "update_nested_toml(\"preprocess.merge_and_split\", \"testing_set_samples\", len(X_test))\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\",\n",
    "    \"train_class_distribution\",\n",
    "    [train_class_distribution[0], train_class_distribution[1]],\n",
    ")\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\",\n",
    "    \"testing_class_distribution\",\n",
    "    [testing_class_distribution[0], testing_class_distribution[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = feature_name\n",
    "X_train[\"label\"] = y_train\n",
    "X_train = X_train.sort_values(by=[\"label\"])\n",
    "train_df = X_train.T\n",
    "train_df.columns = range(train_df.shape[1])\n",
    "\n",
    "X_test.columns = feature_name\n",
    "X_test[\"label\"] = y_test\n",
    "X_test = X_test.sort_values(by=[\"label\"])\n",
    "test_df = X_test.T\n",
    "test_df.columns = range(test_df.shape[1])\n",
    "\n",
    "train_df.insert(0, \"Unnamed: 0\", train_df.index)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.insert(0, \"Unnamed: 0\", test_df.index)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "train_df\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "test_df\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{trainOutPath}\", exist_ok=True)\n",
    "os.makedirs(\n",
    "    f\"{testOutPath}\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "train_df.to_csv(f\"{trainOutPath}/all_beta_normalized_0.csv\", index=False)\n",
    "test_df.to_csv(f\"{testOutPath}/all_beta_normalized_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = \"beta_files.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    zipf.write(\n",
    "        f\"{trainOutPath}/all_beta_normalized_0.csv\", arcname=\"all_beta_normalized_0.csv\"\n",
    "    )\n",
    "    zipf.write(\n",
    "        f\"{testOutPath}/all_beta_normalized_1.csv\", arcname=\"all_beta_normalized_1.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import utils\n",
    "service = utils.authenticate_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utils.create_folder(service, TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.run_upload_with_separate_thread(service, directory, zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the zip from drive\n",
    "utils.download_file(service, directory, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 1 dbeta calculation\n",
    "#### Summary\n",
    "- calculate the difference of beta value between tumor and normal samples\n",
    "- the output file will have the following format\n",
    "  - column 0 is the id of the sample\n",
    "  - column 1 is the gene name\n",
    "  - column 2 is the difference of beta value between tumor and normal samples\n",
    "\n",
    "### Implementation\n",
    "1. split normal and tumor samples\n",
    "2. remove outliers in normal and tumor samples\n",
    "3. calculate the mean of normal sammples\n",
    "4. tumor - avg(normal)\n",
    "5. calculate the mean of tumor samples\n",
    "6. merge with DMP file\n",
    "7. exclude the genes not in single comorbidity list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lung\n",
    "https://drive.google.com/file/d/1BjBach5iyFb0n1DIG6Xc0ru3jMUtmlfW/view?usp=sharing\n",
    "##### Rectal\n",
    "https://drive.google.com/file/d/11DZAwbtqVriSN8EhUNhQEEwVyntiycRa/view?usp=sharing\n",
    "##### Stomach\n",
    "https://drive.google.com/file/d/1QBklKEO61ZYqxo-gfjBsWg3mcREsTRc7/view?usp=sharing\n",
    "##### Prostate\n",
    "tbd\n",
    "##### Breast\n",
    "tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input()\n",
    "\n",
    "output = 'download.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "with zipfile.ZipFile(\"download.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"download\")\n",
    "\n",
    "shutil.move(\"download/all_beta_normalized_0.csv\", f\"all_beta_normalized_0.csv\")\n",
    "shutil.move(\"download/all_beta_normalized_1.csv\", f\"all_beta_normalized_1.csv\")\n",
    "\n",
    "os.remove(\"download.zip\")\n",
    "shutil.rmtree(\"download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{trainOutPath}/all_beta_normalized_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_fence = Q3 + IQR * 1.5\n",
    "    lower_fence = Q1 - IQR * 1.5\n",
    "    return upper_fence, lower_fence\n",
    "\n",
    "\n",
    "def no_outlier(df):\n",
    "    upper_fence, lower_fence = IQR(df)\n",
    "    ddf = df[(df > lower_fence) & (df < upper_fence)]\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "normal_count = config[\"preprocess\"][\"merge_and_split\"][\"train_class_distribution\"][0]\n",
    "all_beta_normalized_normal = train_df.iloc[:-1, 1 : normal_count + 1 :]\n",
    "\n",
    "\n",
    "all_beta_normalized_tumor = train_df.iloc[:-1, normal_count + 1 : :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_normal = no_outlier(all_beta_normalized_normal)\n",
    "all_beta_normalized_tumor = no_outlier(all_beta_normalized_tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_avg = all_beta_normalized_normal.mean(skipna=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor = (all_beta_normalized_tumor).subtract(\n",
    "    train_normal_avg, axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor = no_outlier(all_beta_normalized_tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tumor_mean = all_beta_normalized_tumor.mean(skipna=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta = pd.merge(\n",
    "    train_df.iloc[:-1, :1],\n",
    "    pd.DataFrame(train_tumor_mean, columns=[\"dbeta\"]),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "update_nested_toml(\"preprocess.dbeta\", \"delta_beta_avg\", delta_beta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(delta_beta[pd.isna(delta_beta[\"dbeta\"])])\n",
    "# record the list of feature with dbeta being NaN\n",
    "update_nested_toml(\n",
    "    \"preprocess.dbeta\",\n",
    "    \"NaN_dbeta_feature\",\n",
    "    delta_beta.loc[pd.isna(delta_beta[\"dbeta\"]), \"Unnamed: 0\"].tolist(),\n",
    ")\n",
    "delta_beta.dropna(inplace=True, axis=0)\n",
    "update_nested_toml(\"preprocess.dbeta\", \"delta_beta_avg_remove_NaN\", delta_beta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmp = pd.read_csv(f\"../{cancer_type}/champ_result/{data_source}/DMP_result_0.csv\")\n",
    "dmp = dmp[[\"Unnamed: 0\", \"gene\", \"feature\"]]\n",
    "update_nested_toml(\"preprocess.dbeta\", \"dmp_before_dropna_shape_feature\", dmp.shape[0])\n",
    "dmp.dropna(inplace=True)\n",
    "update_nested_toml(\"preprocess.dbeta\", \"dmp_after_dropna_shape_feature\", dmp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(delta_beta, dmp, on=\"Unnamed: 0\", how=\"left\")\n",
    "update_nested_toml(\n",
    "    \"preprocess.dbeta\", \"delta_beta_avg_remove_NaN_with_gene_name\", result.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dBeta_grouped(group):\n",
    "    idx_max = group[\"dbeta\"].abs().idxmax()\n",
    "    return group.loc[idx_max]\n",
    "\n",
    "\n",
    "dbeta = result.groupby(\"gene\", as_index=False).apply(\n",
    "    find_max_dBeta_grouped, include_groups=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta.columns = [\"gene\", \"ID\", \"dbeta\", \"feature\"]\n",
    "dbeta = dbeta[[\"ID\", \"gene\", \"dbeta\", \"feature\"]]\n",
    "# DEBUG\n",
    "dbeta\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comorbidity = pd.read_csv(\n",
    "#     \"../external_result/matchgene174_single_3Y10__OR2.txt\", sep=\"\\t\", header=None\n",
    "# )\n",
    "# dbeta = dbeta[\n",
    "#     dbeta[\"gene\"].isin(comorbidity[0])\n",
    "# ]\n",
    "\n",
    "# result_max_per_gene_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta.to_csv(f\"{trainOutPath}/dbeta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 2 Filter genes by dbeta values\n",
    "1. filter genes by dbeta values\n",
    "3. filter genes by TSS position\n",
    "4. plot distribution of dbeta values\n",
    "5. plot PCA for normal and tumor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbeta = pd.read_csv(f\"{trainOutPath}/dbeta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS = dbeta[dbeta[\"feature\"].str.contains(\"TSS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS.to_csv(f\"{trainOutPath}/dbeta_TSS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "TSS_threshold = TSS[abs(TSS[\"dbeta\"]) > threshold]\n",
    "while True:\n",
    "    TSS_threshold = TSS[abs(TSS[\"dbeta\"]) > threshold]\n",
    "    count = TSS_threshold.shape[0]\n",
    "    if (\n",
    "        config[\"preprocess\"][\"filtering\"][\"hyper\"][\"avg_dbeta_lower_bound\"]\n",
    "        <= count\n",
    "        <= config[\"preprocess\"][\"filtering\"][\"hyper\"][\"avg_dbeta_upper_bound\"]\n",
    "    ):\n",
    "        break\n",
    "    threshold -= 0.01\n",
    "threshold = round(threshold, 2)\n",
    "update_nested_toml(\"preprocess.filtering\", \"threshold\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS_threshold.to_csv(f\"{trainOutPath}/dbeta_TSS_{threshold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.kdeplot(TSS_threshold[\"dbeta\"])\n",
    "plt.xlabel(\"delta Beta value\")\n",
    "plt.title(\"Density plot of delta Beta value\")\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(f\"{trainOutPath}/all_beta_normalized_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count = (train_df.iloc[-1, 1:] == 0).sum()\n",
    "df_gene = train_df.iloc[:-1, :]\n",
    "df_gene = df_gene[df_gene[df_gene.columns[0]].isin(dbeta[\"ID\"])]\n",
    "X = df_gene.iloc[:, 1:].reset_index(drop=True).T\n",
    "y = [0 if i < normal_count else 1 for i in range(X.shape[0])]\n",
    "# DEBUG\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Principal Component 1\": X_pca[:, 0],\n",
    "        \"Principal Component 2\": X_pca[:, 1],\n",
    "        \"Principal Component 3\": X_pca[:, 2],\n",
    "        \"Class\": y,\n",
    "    }\n",
    ")\n",
    "print(df.shape)\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"Principal Component 1\",\n",
    "    y=\"Principal Component 2\",\n",
    "    z=\"Principal Component 3\",\n",
    "    color=\"Class\",\n",
    "    title=\"PCA of Dataset\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "fig.write_html(f\"{trainOutPath}/preprocess_filtering_pca.html\")\n",
    "# open in browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 Machine Learning\n",
    "1. remove hypo-methylated genes\n",
    "2. RFE\n",
    "3. RFECV (tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "threshold = config[\"preprocess\"][\"filtering\"][\"threshold\"]\n",
    "TSS_threshold = pd.read_csv(f\"{trainOutPath}/dbeta_TSS_{threshold}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS_threshold_hyper = TSS_threshold[TSS_threshold[\"dbeta\"] > 0]\n",
    "# DEBUG\n",
    "TSS_threshold_hyper\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{trainOutPath}/all_beta_normalized_0.csv\")\n",
    "test_df = pd.read_csv(f\"{testOutPath}/all_beta_normalized_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[train_df[\"Unnamed: 0\"].isin(TSS_threshold_hyper[\"ID\"])]\n",
    "X_test = test_df[test_df[\"Unnamed: 0\"].isin(TSS_threshold_hyper[\"ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[:, 1:].T.values.tolist()\n",
    "X_test = X_test.iloc[:, 1:].T.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "normal_count_train = config[\"preprocess\"][\"merge_and_split\"][\n",
    "    \"train_class_distribution\"\n",
    "][0]\n",
    "normal_count_test = config[\"preprocess\"][\"merge_and_split\"][\n",
    "    \"testing_class_distribution\"\n",
    "][0]\n",
    "y_train = [0 if i < normal_count_train else 1 for i in range(len(X_train))]\n",
    "y_test = [0 if i < normal_count_test else 1 for i in range(len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    auc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name):\n",
    "    if name == \"SVM\":\n",
    "        return SVC(kernel=\"linear\")\n",
    "    elif name == \"LR\":\n",
    "        return LogisticRegression()\n",
    "    elif name == \"DT\":\n",
    "        return DecisionTreeClassifier()\n",
    "    elif name == \"RF\":\n",
    "        return RandomForestClassifier()\n",
    "    elif name == \"XGB\":\n",
    "        return XGBClassifier()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {name}\")\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"SVM\",\n",
    "    \"LR\",\n",
    "    \"DT\",\n",
    "    \"RF\",\n",
    "    \"XGB\",\n",
    "]\n",
    "\n",
    "\n",
    "def append_to_file(file_name, data):\n",
    "    if not os.path.isfile(file_name):\n",
    "        data.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        data.to_csv(file_name, index=False, mode=\"a\", header=False)\n",
    "\n",
    "\n",
    "for train_model_name in models:\n",
    "    train_model = get_model(train_model_name)\n",
    "    for feature_count in range(20, 100):\n",
    "        rfe = RFE(estimator=train_model, n_features_to_select=feature_count)\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "        selected_feature_names = (\n",
    "            pd.DataFrame(TSS_threshold_hyper.iloc[rfe.support_, 0])\n",
    "            .reset_index(drop=True)\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        label = f\"{train_model_name}_{feature_count}\"\n",
    "        selected_feature_names.insert(0, \"train_model_name\", label)\n",
    "\n",
    "        append_to_file(f\"{trainOutPath}/selected_feature_names.csv\", selected_feature_names)\n",
    "\n",
    "        for test_model_name in models:\n",
    "            test_model = get_model(test_model_name)\n",
    "            test_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "            # cross_val_score: Evaluate a score by cross-validation with accuracy as the scoring method\n",
    "            train_accuracy_cv = cross_val_score(\n",
    "                test_model, X_train_rfe, y_train, cv=5, scoring=\"accuracy\"\n",
    "            ).mean()\n",
    "\n",
    "            # performace on test set\n",
    "            X_test_rfe = rfe.transform(X_test)\n",
    "            y_pred = test_model.predict(X_test_rfe)\n",
    "            X_test_df = pd.DataFrame(X_test)\n",
    "            incorrect_indices = X_test_df.index[y_pred != y_test]\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "            # 1 is perfect prediction, -1 is imperfect prediction, 0 is equal to random prediction\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            new_performance_row = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"train_model\": train_model_name,\n",
    "                        \"test_model\": test_model_name,\n",
    "                        \"features\": feature_count,\n",
    "                        \"AUC\": roc_auc,\n",
    "                        \"accuracy (5-fold-Cross-Validation)\": train_accuracy_cv,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1_score\": f1,\n",
    "                        \"MCC\": mcc,\n",
    "                        \"J-index\": recall + accuracy - (1 - recall) - 1,\n",
    "                        \"incorrect predictions count\": len(incorrect_indices),\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            append_to_file(f\"{trainOutPath}/results.csv\", new_performance_row)\n",
    "\n",
    "            new_fpr_tpr_row = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"train_model\": train_model_name,\n",
    "                        \"test_model\": test_model_name,\n",
    "                        \"features\": feature_count,\n",
    "                        \"fpr\": fpr,\n",
    "                        \"tpr\": tpr,\n",
    "                        \"AUC\": roc_auc,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            append_to_file(f\"{trainOutPath}/fpr_tpr.csv\", new_fpr_tpr_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
