{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. -1 Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.config_helper import update_nested_toml, load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "breast\n",
    "lung\n",
    "prostate\n",
    "stomach\n",
    "rectal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = input(\"Enter the type of the config file: \")\n",
    "CONFIG_PATH = f\"../config/{TYPE}.toml\"\n",
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_nan(df, name):\n",
    "    print(df[pd.isna(df[name])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_file_number = config[\"init\"][\"hyper\"][\"beta_file_number\"]\n",
    "split_ratio = config[\"init\"][\"hyper\"][\"split_ratio\"]\n",
    "seed = config[\"init\"][\"hyper\"][\"splitting_seed\"]\n",
    "normal_number_0 = config[\"init\"][\"hyper\"][\"normal_number_0\"]\n",
    "if beta_file_number == 2:\n",
    "    normal_number_1 = config[\"init\"][\"hyper\"][\"normal_number_1\"]\n",
    "data_source = config[\"init\"][\"hyper\"][\"data_source\"]\n",
    "is_columns_duplicated = config[\"init\"][\"hyper\"][\"is_columns_duplicated\"]\n",
    "is_oversample = config[\"init\"][\"hyper\"][\"is_oversample\"]\n",
    "majority_df_path = config[\"init\"][\"hyper\"][\"majority_df_path\"]\n",
    "minority_df_path = config[\"init\"][\"hyper\"][\"minority_df_path\"]\n",
    "dmp_file = config[\"init\"][\"hyper\"][\"dmp_file\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_out_path = f\"../{TYPE}/result/{data_source}/split{int(100-split_ratio*100)}\"\n",
    "minority_out_path = f\"../{TYPE}/result/{data_source}/split{int(split_ratio*100)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 0 Merge and Split Champ Data\n",
    "- setting is_oversample = 0 to make sure three datasets are stored properly\n",
    "\n",
    "- file paths\n",
    "  - {TYPE}/result/{data_source}/test20/all_beta_normalized_1.csv\n",
    "  - {TYPE}/result/{data_source}/train80/all_beta_normalized_0.csv\n",
    "  - {TYPE}/result/{data_source}/train80_oversample/all_beta_normalized_0_oversample.csv\n",
    "\n",
    "(in the future, merge part will be removed. directly using the split part to simplify the functionalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Merge Dataset (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(f\"../{TYPE}/champ_result/{data_source}/all_beta_normalized_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df1 = pd.read_csv(f\"../{TYPE}/champ_result/{data_source}/all_beta_normalized_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "df0\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "df1\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential feature loss\n",
    "if beta_file_number == 2:\n",
    "    feature_name_0 = df0.iloc[:, 0].tolist()\n",
    "    feature_name_1 = df1.iloc[:, 0].tolist()\n",
    "\n",
    "    feature_name = list(set(feature_name_0).intersection(feature_name_1))\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_0\", len(feature_name_0)\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_1\", len(feature_name_1)\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_intersection\", len(feature_name)\n",
    "    )\n",
    "elif beta_file_number == 1:\n",
    "    feature_name = df0.iloc[:, 0].tolist()\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"feature_size_0\", len(feature_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df0_join = df0[df0.iloc[:, 0].isin(feature_name)]\n",
    "    df1_join = df1[df1.iloc[:, 0].isin(feature_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df0_join = df0_join.iloc[:, 1::is_columns_duplicated]\n",
    "    df1_join = df1_join.iloc[:, 1::is_columns_duplicated]\n",
    "    df0_join.reset_index(drop=True, inplace=True)\n",
    "    df1_join.reset_index(drop=True, inplace=True)\n",
    "    df0_join_normal = df0_join.iloc[:, :normal_number_0]\n",
    "    df0_join_tumor = df0_join.iloc[:, normal_number_0:]\n",
    "    df1_join_normal = df1_join.iloc[:, :normal_number_1]\n",
    "    df1_join_tumor = df1_join.iloc[:, normal_number_1:]\n",
    "elif beta_file_number == 1:\n",
    "    df0_join = df0.iloc[:, 1::is_columns_duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_file_number == 2:\n",
    "    df_normal = pd.concat([df0_join_normal, df1_join_normal], axis=1)\n",
    "    df_tumor = pd.concat([df0_join_tumor, df1_join_tumor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those samples with missing value\n",
    "# note: could use padding or other methods to fill the missing value\n",
    "\n",
    "if beta_file_number == 2:\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"Before_dropna_dfn_shape\", df_normal.shape\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"Before_dropna_dfc_shape\", df_tumor.shape\n",
    "    )\n",
    "    df_normal.dropna(inplace=True, axis=1)\n",
    "    df_tumor.dropna(inplace=True, axis=1)\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"After_dropna_dfn_shape\", df_normal.shape\n",
    "    )\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"After_dropna_dfc_shape\", df_tumor.shape\n",
    "    )\n",
    "elif beta_file_number == 1:\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"Before_dropna_df_shape\", df0_join.shape\n",
    "    )\n",
    "    df0_join.dropna(inplace=True, axis=1)\n",
    "    update_nested_toml(\n",
    "        \"preprocess.merge_and_split\", \"After_dropna_df_shape\", df0_join.shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the normal and tumor data\n",
    "if beta_file_number == 2:\n",
    "    X = pd.concat([df_normal, df_tumor], axis=1).T\n",
    "    y = [0] * df_normal.shape[1] + [1] * df_tumor.shape[1]\n",
    "elif beta_file_number == 1:\n",
    "    X = df0_join.T\n",
    "    y = [0] * normal_number_0 + [1] * (df0_join.shape[1] - normal_number_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\", \"Before_SMOTE_X_train_shape\", X_train.shape\n",
    ")\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\", \"Before_SMOTE_y_train_shape\", len(y_train)\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=seed)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\", \"After_SMOTE_X_train_shape\", X_resampled.shape\n",
    ")\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\", \"After_SMOTE_y_train_shape\", len(y_resampled)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distribution_oversample = Counter(y_resampled)\n",
    "train_class_distribution = Counter(y_train)\n",
    "testing_class_distribution = Counter(y_test)\n",
    "\n",
    "update_nested_toml(\"preprocess.merge_and_split\", \"training_set_samples_oversample\", len(X_resampled))\n",
    "update_nested_toml(\"preprocess.merge_and_split\", \"training_set_samples\", len(X_train))\n",
    "update_nested_toml(\"preprocess.merge_and_split\", \"testing_set_samples\", len(X_test))\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\", \n",
    "    \"training_class_distribution_oversample\", \n",
    "    [train_class_distribution_oversample[0], train_class_distribution_oversample[1]]\n",
    ")\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\",\n",
    "    \"training_class_distribution\",\n",
    "    [train_class_distribution[0], train_class_distribution[1]],\n",
    ")\n",
    "update_nested_toml(\n",
    "    \"preprocess.merge_and_split\",\n",
    "    \"testing_class_distribution\",\n",
    "    [testing_class_distribution[0], testing_class_distribution[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.columns = feature_name\n",
    "X_resampled[\"label\"] = y_resampled\n",
    "X_resampled.sort_values(by=[\"label\"], inplace=True)\n",
    "train_df_os = X_resampled.T\n",
    "train_df_os.columns = range(train_df_os.shape[1])\n",
    "\n",
    "X_train.columns = feature_name\n",
    "X_train[\"label\"] = y_train\n",
    "X_train.sort_values(by=[\"label\"], inplace=True)\n",
    "train_df = X_train.T\n",
    "train_df.columns = range(train_df.shape[1])\n",
    "\n",
    "X_test.columns = feature_name\n",
    "X_test[\"label\"] = y_test\n",
    "X_test.sort_values(by=[\"label\"], inplace=True)\n",
    "test_df = X_test.T\n",
    "test_df.columns = range(test_df.shape[1])\n",
    "\n",
    "train_df_os.insert(0, \"Unnamed: 0\", train_df_os.index)\n",
    "train_df_os.reset_index(drop=True, inplace=True)\n",
    "train_df.insert(0, \"Unnamed: 0\", train_df.index)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.insert(0, \"Unnamed: 0\", test_df.index)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "train_df\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "test_df\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{trainOutPath}\", exist_ok=True)\n",
    "os.makedirs(\n",
    "    f\"{testOutPath}\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "train_df_os.to_csv(f\"{trainOutPath}_oversample/all_beta_normalized_0_oversample.csv\", index=False)\n",
    "train_df.to_csv(f\"{trainOutPath}/all_beta_normalized_0.csv\", index=False)\n",
    "test_df.to_csv(f\"{testOutPath}/all_beta_normalized_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, test_df, train_df_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.4 Upload Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = f\"{TYPE}_beta_files.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    zipf.write(\n",
    "        f\"{trainOutPath}/all_beta_normalized_0_oversample.csv\",\n",
    "        arcname=\"all_beta_normalized_0_oversample.csv\",\n",
    "    )\n",
    "    zipf.write(\n",
    "        f\"{trainOutPath}/all_beta_normalized_0.csv\", arcname=\"all_beta_normalized_0.csv\"\n",
    "    )\n",
    "    zipf.write(\n",
    "        f\"{testOutPath}/all_beta_normalized_1.csv\", arcname=\"all_beta_normalized_1.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import utils\n",
    "service = utils.authenticate_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utils.create_folder(service, TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.run_upload_with_separate_thread(service, directory, zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 1 Delta Beta Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input()\n",
    "\n",
    "output = 'download.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "with zipfile.ZipFile(\"download.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"download\")\n",
    "\n",
    "shutil.move(\"download/all_beta_normalized_0.csv\", f\"all_beta_normalized_0.csv\")\n",
    "shutil.move(\"download/all_beta_normalized_1.csv\", f\"all_beta_normalized_1.csv\")\n",
    "\n",
    "os.remove(\"download.zip\")\n",
    "shutil.rmtree(\"download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Average Delta Beta Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{majority_out_path}/{majority_set_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier in terms of every column\n",
    "def IQR(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_fence = Q3 + IQR * 1.5\n",
    "    lower_fence = Q1 - IQR * 1.5\n",
    "    return upper_fence, lower_fence\n",
    "\n",
    "\n",
    "def no_outlier(df):\n",
    "    upper_fence, lower_fence = IQR(df)\n",
    "    ddf = df[(df > lower_fence) & (df < upper_fence)]\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normal count by the count of 0 in the last row\n",
    "normal_count = int((train_df.iloc[-1, 1:] == 0).sum())\n",
    "all_beta_normalized_normal = train_df.iloc[:-1, 1 : normal_count + 1 :].T\n",
    "\n",
    "all_beta_normalized_tumor = train_df.iloc[:-1, normal_count + 1 : :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_normal = no_outlier(all_beta_normalized_normal)\n",
    "all_beta_normalized_tumor = no_outlier(all_beta_normalized_tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_avg = all_beta_normalized_normal.mean(skipna=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor = all_beta_normalized_tumor.subtract(\n",
    "    train_normal_avg, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor = no_outlier(all_beta_normalized_tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tumor_mean = all_beta_normalized_tumor.mean(skipna=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta = pd.merge(\n",
    "    train_df.iloc[:-1, :1],\n",
    "    pd.DataFrame(train_tumor_mean, columns=[\"dbeta\"]),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "update_nested_toml(\"preprocess.dbeta\", \"delta_beta_avg\", delta_beta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(delta_beta[pd.isna(delta_beta[\"dbeta\"])])\n",
    "# record the list of feature with dbeta being NaN\n",
    "update_nested_toml(\n",
    "    \"preprocess.dbeta\",\n",
    "    \"NaN_dbeta_feature\",\n",
    "    delta_beta.loc[pd.isna(delta_beta[\"dbeta\"]), \"Unnamed: 0\"].tolist(),\n",
    ")\n",
    "delta_beta.dropna(inplace=True, axis=0)\n",
    "update_nested_toml(\"preprocess.dbeta\", \"delta_beta_avg_remove_NaN\", delta_beta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmp = pd.read_csv(f\"../{TYPE}/champ_result/{data_source}/{dmp_file}\")\n",
    "dmp = dmp[[\"Unnamed: 0\", \"gene\", \"feature\"]]\n",
    "update_nested_toml(\"preprocess.dbeta\", \"dmp_before_dropna_shape_feature\", dmp.shape[0])\n",
    "dmp.dropna(inplace=True)\n",
    "update_nested_toml(\"preprocess.dbeta\", \"dmp_after_dropna_shape_feature\", dmp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(delta_beta, dmp, on=\"Unnamed: 0\", how=\"left\")\n",
    "update_nested_toml(\n",
    "    \"preprocess.dbeta\", \"delta_beta_avg_remove_NaN_with_gene_name\", result.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dBeta_grouped(group):\n",
    "    idx_max = group[\"dbeta\"].abs().idxmax()\n",
    "    return group.loc[idx_max]\n",
    "\n",
    "\n",
    "dbeta_info = result.groupby(\"gene\", as_index=False).apply(\n",
    "    find_max_dBeta_grouped, include_groups=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info.columns = [\"gene\", \"ID\", \"dbeta\", \"feature\"]\n",
    "dbeta_info = dbeta_info[[\"ID\", \"gene\", \"dbeta\", \"feature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "dbeta_info\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comorbidity = pd.read_csv(\n",
    "#     \"../external_result/matchgene174_single_3Y10__OR2.txt\", sep=\"\\t\", header=None\n",
    "# )\n",
    "# dbeta = dbeta[\n",
    "#     dbeta[\"gene\"].isin(comorbidity[0])\n",
    "# ]\n",
    "\n",
    "# result_max_per_gene_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info[\"dbeta\"] = dbeta_info[\"dbeta\"].apply(lambda x: round(x, 6))\n",
    "dbeta_info.to_csv(f\"{majority_out_path}/dbeta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 2 Filter Genes by Average Delta Beta Values\n",
    "1. filter genes by dbeta values\n",
    "3. filter genes by TSS position\n",
    "4. plot distribution of dbeta values\n",
    "5. plot PCA for normal and tumor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Filtering TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbeta = pd.read_csv(f\"{majority_out_path}/dbeta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS = dbeta_info[dbeta_info[\"feature\"].str.contains(\"TSS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS.to_csv(f\"{majority_out_path}/dbeta_TSS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "dbeta_TSS_threshold = TSS[abs(TSS[\"dbeta\"]) > threshold]\n",
    "while True:\n",
    "    dbeta_TSS_threshold = TSS[abs(TSS[\"dbeta\"]) > threshold]\n",
    "    count = dbeta_TSS_threshold.shape[0]\n",
    "    if (\n",
    "        config[\"preprocess\"][\"filtering\"][\"hyper\"][\"avg_dbeta_lower_bound\"]\n",
    "        <= count\n",
    "        <= config[\"preprocess\"][\"filtering\"][\"hyper\"][\"avg_dbeta_upper_bound\"]\n",
    "    ):\n",
    "        break\n",
    "    threshold -= 0.01\n",
    "threshold = round(threshold, 2)\n",
    "update_nested_toml(\"preprocess.filtering\", \"threshold\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_TSS_threshold.to_csv(f\"{majority_out_path}/dbeta_TSS_{threshold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.kdeplot(dbeta_TSS_threshold[\"dbeta\"])\n",
    "plt.xlabel(\"delta Beta value\")\n",
    "plt.title(\"Density plot of delta Beta value\")\n",
    "plt.savefig(f\"{majority_out_path}/dbeta_TSS_{threshold}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(f\"{trainOutPath}/all_beta_normalized_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count = (train_df.iloc[-1, 1:] == 0).sum()\n",
    "df_gene = train_df.iloc[:-1, :]\n",
    "df_gene = df_gene[df_gene[df_gene.columns[0]].isin(dbeta[\"ID\"])]\n",
    "X = df_gene.iloc[:, 1:].reset_index(drop=True).T\n",
    "y = [0 if i < normal_count else 1 for i in range(X.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Principal Component 1\": X_pca[:, 0],\n",
    "        \"Principal Component 2\": X_pca[:, 1],\n",
    "        \"Principal Component 3\": X_pca[:, 2],\n",
    "        \"Class\": y,\n",
    "    }\n",
    ")\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"Principal Component 1\",\n",
    "    y=\"Principal Component 2\",\n",
    "    z=\"Principal Component 3\",\n",
    "    color=\"Class\",\n",
    "    title=\"PCA of Dataset\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{majority_out_path}/preprocess_filtering_pca.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 Feature Selection with ML (SFS)\n",
    "sequential forward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove previous results\n",
    "Warning: This step is not reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "if os.path.exists(f\"{majority_out_path}/sfs\"):\n",
    "    shutil.rmtree(f\"{majority_out_path}/sfs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"machine_learning\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{majority_out_path}/{dbeta_info_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSS_threshold_hyper = TSS_threshold[TSS_threshold[\"dbeta\"] > 0]\n",
    "# # DEBUG\n",
    "# TSS_threshold_hyper\n",
    "# # END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if logs/ folder exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "from utils.train_helper import TrainHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(dbeta_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{majority_out_path}/{majority_df_path}\")\n",
    "validate_df = pd.read_csv(f\"{minority_out_path}/{minority_df_path}\")\n",
    "th.set_train_validate_df(train_df, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "th.set_train_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO Training SVM with SFS\n",
      "INFO Training SVM with 3 clusters selected\n",
      "INFO Training finished with 4 clusters selected\n",
      "INFO Training DecisionTree with SFS\n",
      "INFO Training DecisionTree with 3 clusters selected\n",
      "INFO Training finished with 4 clusters selected\n",
      "INFO Training RandomForest with SFS\n",
      "INFO Training RandomForest with 3 clusters selected\n",
      "INFO Training finished with 4 clusters selected\n",
      "INFO Training XGBoost with SFS\n",
      "INFO Training XGBoost with 3 clusters selected\n",
      "INFO Training finished with 4 clusters selected\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{majority_out_path}/sfs\", exist_ok=True)\n",
    "\n",
    "th.select_feature_sfs(\n",
    "    out_path = f\"{majority_out_path}/sfs/selected_feature.txt\",\n",
    "    step= 4,\n",
    "    n_features_to_select=\"cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 feature Selection with ML (RFE)\n",
    "recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove previous results\n",
    "Warning: This step is not reversible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "if os.path.exists(f\"{majority_out_path}/rfe\"):\n",
    "    shutil.rmtree(f\"{majority_out_path}/rfe\")\n",
    "if os.path.exists(f\"{minority_out_path}/rfe\"):\n",
    "    shutil.rmtree(f\"{minority_out_path}/rfe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "folder = \"rfe\"\n",
    "threshold_file = config[\"machine_learning\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "\n",
    "TSS_threshold = pd.read_csv(f\"{majority_out_path}/{threshold_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "from utils.train_helper import TrainHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(TSS_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{majority_out_path}/{majority_df_path}\")\n",
    "validate_df = pd.read_csv(f\"{minority_out_path}/{minority_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_train_validate_df(train_df, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import set_parameters\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f\"{majority_out_path}/training_param.json\", \"r\") as f:\n",
    "    training_param = json.load(f)\n",
    "xgb_grid = set_parameters(XGBClassifier(random_state=42), training_param[\"XGBoost\"])\n",
    "rf_grid = set_parameters(\n",
    "    RandomForestClassifier(random_state=42), training_param[\"RandomForest\"]\n",
    ")\n",
    "svm_grid = set_parameters(SVC(random_state=42, probability=True), training_param[\"SVM\"])\n",
    "dt_grid = set_parameters(\n",
    "    DecisionTreeClassifier(random_state=42), training_param[\"DecisionTree\"]\n",
    ")\n",
    "\n",
    "train_models = {\n",
    "    \"XGBoost\": xgb_grid,\n",
    "    \"RandomForest\": rf_grid,\n",
    "    \"SVM\": svm_grid,\n",
    "    \"DecisionTree\": dt_grid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "th.set_grid_estimators(train_models)\n",
    "th.set_train_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{majority_out_path}/{folder}\", exist_ok=True)\n",
    "os.makedirs(f\"{minority_out_path}/{folder}\", exist_ok=True)\n",
    "\n",
    "th.select_feture_rfe(\n",
    "    train_out_path = f\"{majority_out_path}/{folder}\",\n",
    "    validate_out_path = f\"{minority_out_path}/{folder}\",\n",
    "    selected_feature_path = f\"{majority_out_path}/{folder}/selected_feature.txt\",\n",
    "    feature_range = (1, 6, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 4 Clean Selected Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate Feature json for SimpleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'SVM': ['APOC3', 'ATG3', 'FBRSL1', 'RNH1', 'C15orf2'],\n",
       "             'DecisionTree': ['C2orf89', 'MME', 'IKBKB', 'MNX1', 'C1orf212'],\n",
       "             'RandomForest': ['ATG3', 'ICOSLG', 'PTPRG', 'MME', 'IRF7'],\n",
       "             'XGBoost': ['ACCN4', 'DNMT1', 'FXR2', 'MME', 'APBB1', 'ARRB2']})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"machine_learning\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{majority_out_path}/{dbeta_info_file}\")\n",
    "th = TrainHelper(dbeta_info)\n",
    "\n",
    "folder = \"sfs\"\n",
    "features = read_selected_features(f\"{majority_out_path}/{folder}/selected_feature.txt\")\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{majority_out_path}/{folder}/selected_features.json\",\n",
    "    mode=\"min\",\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "# use this to read json\n",
    "read_selected_features_json(f\"{majority_out_path}/{folder}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Gather Selected gene list from best selection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"rfe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_train = pd.read_csv(f\"{majority_out_path}/{folder}/rfe.csv\")\n",
    "rfe_validate = pd.read_csv(f\"{minority_out_path}/{folder}/rfe.csv\")\n",
    "fpr_tpr_train = pd.read_csv(f\"{majority_out_path}/{folder}/roc_curve.csv\")\n",
    "fpr_tpr_validate = pd.read_csv(f\"{minority_out_path}/{folder}/roc_curve.csv\")\n",
    "rfe_j = pd.merge(rfe_train, rfe_validate, on=[\"selection_model\", \"train_model\", \"features\"], suffixes=('_train', '_validate'))\n",
    "fpr_tpr_j = pd.merge(fpr_tpr_train, fpr_tpr_validate, on=[\"selection_model\", \"train_model\", \"features\"], suffixes=('_train', '_validate'))\n",
    "J = pd.merge(rfe_j, fpr_tpr_j, on=[\"selection_model\", \"train_model\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "J[\"fpr_train\"] = J[\"fpr_train\"].apply(ast.literal_eval)\n",
    "J[\"tpr_train\"] = J[\"tpr_train\"].apply(ast.literal_eval)\n",
    "J[\"fpr_validate\"] = J[\"fpr_validate\"].apply(ast.literal_eval)\n",
    "J[\"tpr_validate\"] = J[\"tpr_validate\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.painter import plot_roc_curve, create_performance_barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "J['accuracy_diff'] = J['accuracy_train'] - J['accuracy_validate']\n",
    "J['recall_diff'] = J['recall_train'] - J['recall_validate']\n",
    "J['f1_score_diff'] = J['f1_score_train'] - J['f1_score_validate']\n",
    "J['AUC_diff'] = J['AUC_train'] - J['AUC_validate']\n",
    "J['MCC_diff'] = J['MCC_train'] - J['MCC_validate']\n",
    "J['fbeta2_score_diff'] = J['fbeta2_score_train'] - J['fbeta2_score_validate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve saved to ../lung/result/GDC_lung_tissue/split80/rfe/roc_curve.html\n",
      "ROC curve saved to ../lung/result/GDC_lung_tissue/split20/rfe/roc_curve.html\n"
     ]
    }
   ],
   "source": [
    "# tweakable width and height\n",
    "plot_roc_curve(\n",
    "    J, \n",
    "    \"ROC Curves on Training Set\", \n",
    "    f\"{majority_out_path}/{folder}/roc_curve.html\",\n",
    "    x_column = \"fpr_train\",\n",
    "    y_column = \"tpr_train\",\n",
    "    trace_name = [\"selection_model\", \"train_model\", \"features\"],\n",
    ")\n",
    "# tweakable width and height\n",
    "plot_roc_curve(\n",
    "    J, \n",
    "    \"ROC Curves on Testing Set\", \n",
    "    f\"{minority_out_path}/{folder}/roc_curve.html\",\n",
    "    x_column = \"fpr_validate\",\n",
    "    y_column = \"tpr_validate\",\n",
    "    trace_name = [\"selection_model\", \"train_model\", \"features\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance difference saved to ../lung/result/GDC_lung_tissue/split80/rfe/performance_diff_grouped_by_train_model.html\n"
     ]
    }
   ],
   "source": [
    "# plot difference\n",
    "performance_metrics = ['accuracy_diff', 'recall_diff', 'f1_score_diff', 'AUC_diff', 'MCC_diff', 'fbeta2_score_diff']\n",
    "ground_by_train_model = J.groupby('train_model')[performance_metrics].mean()\n",
    "ground_by_train_model['train_model'] = ground_by_train_model.index\n",
    "ground_by_train_model.to_csv(f\"{majority_out_path}/{folder}/performance_diff_grouped_by_train_model.csv\", index=False)\n",
    "color_mapping = {\n",
    "    \"accuracy_diff\": \"blue\",\n",
    "    \"recall_diff\": \"red\",\n",
    "    \"f1_score_diff\": \"green\",\n",
    "    \"AUC_diff\": \"purple\",\n",
    "    \"MCC_diff\": \"orange\",\n",
    "    \"fbeta2_score_diff\": \"brown\",\n",
    "}\n",
    "create_performance_barchart(\n",
    "    df=ground_by_train_model,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"train_model\",\n",
    "    out_path=f\"{majority_out_path}/{folder}/performance_diff_grouped_by_train_model.html\",\n",
    "    title=\"Grouped Performance Difference between Training and Testing Set\",\n",
    "    x_axis_label=\"Performance Difference (Training - Testing)\",\n",
    "    y_axis_label=\"Train Model\",\n",
    "    orientation=\"h\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J[[\"selection_model\", \"train_model\", \"features\", \"accuracy_validate\", \"recall_validate\", \"f1_score_validate\", \"AUC_validate\", \"MCC_validate\", \"fbeta2_score_validate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance difference saved to ../lung/result/GDC_lung_tissue/split80/rfe/performance_metrics_grouped_by_train_model.html\n",
      "Best train model: SVM\n",
      "Performance difference saved to ../lung/result/GDC_lung_tissue/split80/rfe/performance_metrics_grouped_by_feature.html\n",
      "Best number of feature: 5\n"
     ]
    }
   ],
   "source": [
    "# group by train_model, for each train_model, calculate the mean of each performance metric\n",
    "performance_metrics = ['accuracy_validate', 'recall_validate',\n",
    "                       'f1_score_validate', 'AUC_validate', 'MCC_validate']\n",
    "ground_by_train_model = J.groupby('train_model')[performance_metrics].mean()\n",
    "ground_by_train_model['train_model'] = ground_by_train_model.index\n",
    "ground_by_train_model.to_csv(\n",
    "    f\"{majority_out_path}/{folder}/performance_metrics_grouped_by_train_model.csv\", index=False)\n",
    "color_mapping = {\n",
    "    \"accuracy_validate\": \"blue\",\n",
    "    \"recall_validate\": \"red\",\n",
    "    \"f1_score_validate\": \"green\",\n",
    "    \"AUC_validate\": \"purple\",\n",
    "    \"MCC_validate\": \"orange\",\n",
    "}\n",
    "create_performance_barchart(\n",
    "    df=ground_by_train_model,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"train_model\",\n",
    "    out_path=f\"{majority_out_path}/{folder}/performance_metrics_grouped_by_train_model.html\",\n",
    "    title=\"Grouped Performance Metrics by Train Model\",\n",
    "    x_axis_label=\"Performance\",\n",
    "    y_axis_label=\"Train Model\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "best_train_model = ground_by_train_model['MCC_validate'].idxmax()\n",
    "print(f\"Best train model: {best_train_model}\")\n",
    "ground_by_feature = J[J['train_model'] == best_train_model].groupby('features')[\n",
    "    performance_metrics].mean()\n",
    "ground_by_feature['features'] = ground_by_feature.index\n",
    "ground_by_feature.to_csv(\n",
    "    f\"{majority_out_path}/{folder}/performance_metrics_grouped_by_feature.csv\", index=False)\n",
    "create_performance_barchart(\n",
    "    df=ground_by_feature,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"features\",\n",
    "    out_path=f\"{majority_out_path}/{folder}/performance_metrics_grouped_by_feature.html\",\n",
    "    title=\"Grouped Performance Metrics by Feature\",\n",
    "    x_axis_label=\"Performance\",\n",
    "    y_axis_label=\"Feature\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "best_num_of_feature = ground_by_feature['MCC_validate'].idxmax()\n",
    "print(f\"Best number of feature: {best_num_of_feature}\")\n",
    "best_performance_records = J[(J['train_model'] == best_train_model) & (\n",
    "    J['features'] == best_num_of_feature)]\n",
    "best_performance_records.to_csv(\n",
    "    f\"{majority_out_path}/{folder}/best_performance_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'best': ['ATG3',\n",
       "              'BOD1',\n",
       "              'TMEM159',\n",
       "              'TMEM159',\n",
       "              'VAC14',\n",
       "              'ATG3',\n",
       "              'DHX8',\n",
       "              'HIST1H1C',\n",
       "              'ZBTB25',\n",
       "              'TRNP1',\n",
       "              'ZBTB25',\n",
       "              'SMG6',\n",
       "              'SRP68',\n",
       "              'MME',\n",
       "              'TMEM108',\n",
       "              'ALDH1A3',\n",
       "              'MME',\n",
       "              'PTPRG',\n",
       "              'RNH1',\n",
       "              'MX1']})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"machine_learning\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{majority_out_path}/{dbeta_info_file}\")\n",
    "th = TrainHelper(dbeta_info)\n",
    "\n",
    "features = read_selected_features(f\"{majority_out_path}/{folder}/selected_feature.txt\")\n",
    "\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{majority_out_path}/{folder}/selected_features.json\",\n",
    "    mode=int(best_num_of_feature),\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "# use this to read json\n",
    "read_selected_features_json(f\"{majority_out_path}/{folder}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 5 Clustering Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data\n",
    "\n",
    "remember to calculate distance matrix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.clustering_helper import hierarchical_clustering, check_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_path = config[\"clustering\"][\"hyper\"][\"dbeta_file\"]\n",
    "gene_set_file = config[\"clustering\"][\"hyper\"][\"gene_set_file\"]\n",
    "bp_file = config[\"clustering\"][\"hyper\"][\"bp_file\"]\n",
    "cc_file = config[\"clustering\"][\"hyper\"][\"cc_file\"]\n",
    "mf_file = config[\"clustering\"][\"hyper\"][\"mf_file\"]\n",
    "terms_count_file = config[\"clustering\"][\"hyper\"][\"terms_count_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_set = pd.read_csv(f\"{majority_out_path}/{gene_set_file}\", index_col=0)\n",
    "distance_matrix_bp = pd.read_csv(f\"{majority_out_path}/{bp_file}\", index_col=0)\n",
    "distance_matrix_cc = pd.read_csv(f\"{majority_out_path}/{cc_file}\", index_col=0)\n",
    "distance_matrix_mf = pd.read_csv(f\"{majority_out_path}/{mf_file}\", index_col=0)\n",
    "terms_count = pd.read_csv(f\"{majority_out_path}/{terms_count_file}\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 0\n",
    "distance_matrix_bp = distance_matrix_bp.fillna(0)\n",
    "distance_matrix_cc = distance_matrix_cc.fillna(0)\n",
    "distance_matrix_mf = distance_matrix_mf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex distance matrix\n",
    "index_bp = distance_matrix_bp.index\n",
    "index_cc = distance_matrix_cc.index\n",
    "index_mf = distance_matrix_mf.index\n",
    "index = index_bp.union(index_cc).union(index_mf)\n",
    "distance_matrix_bp_ = distance_matrix_bp.reindex(index=index, columns=index, fill_value=0)\n",
    "distance_matrix_cc_ = distance_matrix_cc.reindex(index=index, columns=index, fill_value=0)\n",
    "distance_matrix_mf_ = distance_matrix_mf.reindex(index=index, columns=index, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a array of distance matrix for each ontology\n",
    "distance_matrix = []\n",
    "\n",
    "distance_matrix.append(distance_matrix_bp_)\n",
    "distance_matrix.append(distance_matrix_cc_)\n",
    "distance_matrix.append(distance_matrix_mf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Weighted Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [count for count in terms_count[\"count\"]]\n",
    "weight = weight / np.sum(weight)\n",
    "masks = np.array([~np.isnan(distance_matrix[i].values) for i in range(3)])\n",
    "\n",
    "valid_weights = np.array([weight[i] for i in range(3)])[:, None, None] * masks\n",
    "\n",
    "weight_sums = valid_weights.sum(axis=0)\n",
    "\n",
    "normalized_weights = np.divide(valid_weights, weight_sums, where=weight_sums != 0)\n",
    "weighted_sum = sum(\n",
    "    np.nan_to_num(distance_matrix[i].values) * normalized_weights[i] for i in range(3)\n",
    ")\n",
    "\n",
    "\n",
    "weighted_sum_dataframe = pd.DataFrame(weighted_sum, index=index, columns=index)\n",
    "\n",
    "weighted_sum_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_weighted = hierarchical_clustering(\n",
    "    weighted_sum_dataframe,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{majority_out_path}/hierarchical_clustering_weighted_sum.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_weighted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [1, 1, 1]\n",
    "masks = np.array([~np.isnan(distance_matrix[i].values) for i in range(3)])\n",
    "valid_weights = np.array([weight[i] for i in range(3)])[:, None, None] * masks\n",
    "weight_sums = valid_weights.sum(axis=0)\n",
    "normalized_weights = np.divide(valid_weights, weight_sums, where=weight_sums != 0)\n",
    "weighted_sum = sum(\n",
    "    np.nan_to_num(distance_matrix[i].values) * normalized_weights[i] for i in range(3)\n",
    ")\n",
    "simple_sum_dataframe = pd.DataFrame(weighted_sum, index=index, columns=index)\n",
    "simple_sum_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_simple = hierarchical_clustering(\n",
    "    simple_sum_dataframe,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{majority_out_path}/hierarchical_clustering_simple_sum.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Consensus clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bp = hierarchical_clustering(\n",
    "    distance_matrix_bp, out_path=f\"{trainOutPath}/hierarchical_clustering_bp.png\"\n",
    ")\n",
    "cluster_cc = hierarchical_clustering(\n",
    "    distance_matrix_cc, out_path=f\"{trainOutPath}/hierarchical_clustering_cc.png\"\n",
    ")\n",
    "cluster_mf = hierarchical_clustering(\n",
    "    distance_matrix_mf, out_path=f\"{trainOutPath}/hierarchical_clustering_mf.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bp.columns = [\"gene\", \"cluster_bp\"]\n",
    "cluster_cc.columns = [\"gene\", \"cluster_cc\"]\n",
    "cluster_mf.columns = [\"gene\", \"cluster_mf\"]\n",
    "cluster_bp_cc = pd.merge(cluster_bp, cluster_cc, on=\"gene\", how=\"outer\")\n",
    "cluster_go = pd.merge(cluster_bp_cc, cluster_mf, on=\"gene\", how=\"outer\")\n",
    "cluster_go = cluster_go.fillna(-1)\n",
    "print(cluster_go.shape)\n",
    "cluster_go.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = cluster_go.shape[0]\n",
    "consensus_matrix = np.zeros((num_genes, num_genes))\n",
    "\n",
    "for i in range(num_genes):\n",
    "    for j in range(i, num_genes):\n",
    "        if cluster_go.iloc[i][\"cluster_bp\"] == cluster_go.iloc[j][\"cluster_bp\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "        if cluster_go.iloc[i][\"cluster_cc\"] == cluster_go.iloc[j][\"cluster_cc\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "        if cluster_go.iloc[i][\"cluster_mf\"] == cluster_go.iloc[j][\"cluster_mf\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "\n",
    "consensus_matrix = pd.DataFrame(\n",
    "    consensus_matrix, index=cluster_go[\"gene\"], columns=cluster_go[\"gene\"]\n",
    ")\n",
    "consensus_matrix += consensus_matrix.T\n",
    "\n",
    "\n",
    "distance_matrix_consensus = 1 - consensus_matrix / 3\n",
    "np.fill_diagonal(distance_matrix_consensus.values, 0)\n",
    "\n",
    "\n",
    "distance_matrix_consensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_consensus = hierarchical_clustering(\n",
    "    distance_matrix_consensus,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=4,\n",
    "    out_path=f\"{trainOutPath}/hierarchical_clustering_consensus.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_consensus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering_helper import hierarchical_clustering_compare\n",
    "\n",
    "hierarchical_clustering_compare(\n",
    "    [weighted_sum_dataframe, simple_sum_dataframe, distance_matrix_consensus],\n",
    "    [\"Weighted Average\", \"Simple Average\", \"Consensus\"],\n",
    "    out_path=f\"{trainOutPath}/hierarchical_clustering_compare.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta[\"ID\"] = dbeta.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column gene isin weighted_sum_dataframe\n",
    "weighted_dbeta = dbeta[dbeta[\"gene\"].isin(weighted_sum_dataframe.index)]\n",
    "simple_dbeta = dbeta[dbeta[\"gene\"].isin(simple_sum_dataframe.index)]\n",
    "consensus_dbeta = dbeta[dbeta[\"gene\"].isin(distance_matrix_consensus.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_dbeta.merge(cluster_result_weighted, on=\"gene\").to_csv(\n",
    "    f\"{trainOutPath}/{dbeta_path}_weighted.csv\", index=False\n",
    ")\n",
    "simple_dbeta.merge(cluster_result_simple, on=\"gene\").to_csv(\n",
    "    f\"{trainOutPath}/{dbeta_path}_simple.csv\", index=False\n",
    ")\n",
    "consensus_dbeta.merge(cluster_result_consensus, on=\"gene\").to_csv(\n",
    "    f\"{trainOutPath}/{dbeta_path}_consensus.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 6 SimpleModel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = input(\"Enter either sfs or rfe:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features_json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{majority_out_path}/{majority_df_path}\")\n",
    "test_df = pd.read_csv(f\"{minority_out_path}/{minority_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_dict = read_selected_features_json(f\"{majority_out_path}/{folder}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_path = config[\"combination\"][\"hyper\"][\"dbeta_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{majority_out_path}/{dbeta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import set_parameters\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import json\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "with open(f\"{majority_out_path}/training_param.json\", \"r\") as f:\n",
    "    training_param = json.load(f)\n",
    "\n",
    "xgb_grid = set_parameters(XGBClassifier(random_state=42), training_param[\"XGBoost\"])\n",
    "rf_grid = set_parameters(RandomForestClassifier(random_state=42), training_param[\"RandomForest\"])\n",
    "svm_grid = set_parameters(SVC(random_state=42, probability=True), training_param[\"SVM\"])\n",
    "dt_grid = set_parameters(DecisionTreeClassifier(random_state=42), training_param[\"DecisionTree\"])\n",
    "voting = VotingClassifier(\n",
    "    estimators=[(\"XGBoost\", XGBClassifier(random_state=42)), (\"RandomForest\", RandomForestClassifier(random_state=42)), (\"SVM\", SVC(random_state=42, probability=True)), (\"DecisionTree\", DecisionTreeClassifier(random_state=42))\n",
    "                ],\n",
    "    voting=\"soft\",\n",
    ")\n",
    "\n",
    "# comment out the model you don't want to use\n",
    "models = {\n",
    "    \"XGBoost\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": xgb_grid,\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": rf_grid,\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": svm_grid,\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": dt_grid,\n",
    "    },\n",
    "    \"Voting\": {\n",
    "        \"is_grid_search\": False,\n",
    "        \"model\": voting,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO Training for combination: ('APOC3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ATG3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repo\\btc\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO Training for combination: ('APOC3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ATG3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('APOC3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ATG3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('APOC3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ATG3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('APOC3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: Voting\n",
      "INFO Training for combination: ('ATG3', 'C15orf2', 'FBRSL1', 'RNH1') with estimator: Voting\n",
      "INFO Training for combination: ('C1orf212', 'C2orf89', 'IKBKB', 'MNX1') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repo\\btc\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO Training for combination: ('C1orf212', 'MME', 'IKBKB', 'MNX1') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('C1orf212', 'C2orf89', 'IKBKB', 'MNX1') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('C1orf212', 'MME', 'IKBKB', 'MNX1') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('C1orf212', 'C2orf89', 'IKBKB', 'MNX1') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('C1orf212', 'MME', 'IKBKB', 'MNX1') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('C1orf212', 'C2orf89', 'IKBKB', 'MNX1') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('C1orf212', 'MME', 'IKBKB', 'MNX1') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('C1orf212', 'C2orf89', 'IKBKB', 'MNX1') with estimator: Voting\n",
      "INFO Training for combination: ('C1orf212', 'MME', 'IKBKB', 'MNX1') with estimator: Voting\n",
      "INFO Training for combination: ('ATG3', 'ICOSLG', 'IRF7', 'MME') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ATG3', 'PTPRG', 'IRF7', 'MME') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ATG3', 'ICOSLG', 'IRF7', 'MME') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ATG3', 'PTPRG', 'IRF7', 'MME') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ATG3', 'ICOSLG', 'IRF7', 'MME') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ATG3', 'PTPRG', 'IRF7', 'MME') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ATG3', 'ICOSLG', 'IRF7', 'MME') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ATG3', 'PTPRG', 'IRF7', 'MME') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ATG3', 'ICOSLG', 'IRF7', 'MME') with estimator: Voting\n",
      "INFO Training for combination: ('ATG3', 'PTPRG', 'IRF7', 'MME') with estimator: Voting\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'DNMT1', 'MME') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'FXR2', 'MME') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'DNMT1', 'MME') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'FXR2', 'MME') with estimator: XGBoost\n",
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'DNMT1', 'MME') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'FXR2', 'MME') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'DNMT1', 'MME') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'FXR2', 'MME') with estimator: RandomForest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'DNMT1', 'MME') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'FXR2', 'MME') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'DNMT1', 'MME') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'FXR2', 'MME') with estimator: SVM\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'DNMT1', 'MME') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'FXR2', 'MME') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'DNMT1', 'MME') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'FXR2', 'MME') with estimator: DecisionTree\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'DNMT1', 'MME') with estimator: Voting\n",
      "INFO Training for combination: ('ACCN4', 'APBB1', 'FXR2', 'MME') with estimator: Voting\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'DNMT1', 'MME') with estimator: Voting\n",
      "INFO Training for combination: ('ACCN4', 'ARRB2', 'FXR2', 'MME') with estimator: Voting\n"
     ]
    }
   ],
   "source": [
    "from utils.simple_model import SimpleModel\n",
    "\n",
    "for model_name, gene_list in gene_dict.items():\n",
    "    for model_name, model_config in models.items():\n",
    "        model = SimpleModel(\n",
    "            train_df=train_df,\n",
    "            test_df=test_df,\n",
    "            gene_list=gene_list,\n",
    "            dbeta_info=dbeta_info,\n",
    "            method=\"sfs\"\n",
    "        )\n",
    "        model.setup_dbeta()\n",
    "        model.setup_train_test()\n",
    "        model.setup_combinations()\n",
    "        model.train(\n",
    "            model_name,\n",
    "            model_config[\"model\"],\n",
    "            f\"{majority_out_path}/sm/\",\n",
    "            f\"{minority_out_path}/sm/\",\n",
    "            model_config[\"is_grid_search\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
