{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. -1 Initiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.config_helper import update_nested_toml, load_config\n",
    "os.makedirs(\"logs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE = input(\"Enter the type of the config file: \")\n",
    "TYPE = \"prostate\"\n",
    "CONFIG_PATH = f\"../config/{TYPE}.toml\"\n",
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_file = config[\"init\"][\"hyper\"][\"train_df_file\"]\n",
    "dmp_files = config[\"init\"][\"hyper\"][\"dmp_files\"]\n",
    "majority_out_path = config[\"init\"][\"hyper\"][\"majority_out_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 0 process norm examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split Dataset Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.process_norm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "# cancer_type = input(\"Enter the cancer type: \")\n",
    "cancer_type = \"prostate\"\n",
    "with open(f\"../config/{cancer_type}.json\") as f:\n",
    "    J = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_source = input(\"Enter the raw all_beta_normalized file: \")\n",
    "data_source = \"GSE269244\"\n",
    "df = pd.read_csv(J[data_source]['file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = list(df.iloc[:, 0]) + [\"label\"]\n",
    "sample_count = 2\n",
    "normal_count = 50\n",
    "tumor_count = 502\n",
    "X = df.iloc[:, 1::sample_count].T\n",
    "print(X.shape[0],normal_count,tumor_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = organize_dataset(df, J[data_source][\"normal\"], J[data_source][\"tumor\"], J[data_source][\"sample_count\"])\n",
    "complement_df, ratio_df = split_dataset(dfo, J[data_source][\"split_test\"], J[data_source][\"random_state\"])\n",
    "complement_df.to_csv(\"all_beta_normalized_80.csv\", index=False)\n",
    "ratio_df.to_csv(\"all_beta_normalized_20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Merge Dataset Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(f\"../stomach/champ_result/gdc_stomach_GSE99553/all_beta_normalized_0.csv\")\n",
    "df1 = pd.read_csv(f\"../stomach/champ_result/gdc_stomach_GSE99553/all_beta_normalized_GSE99553.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = organize_dataset(df0, 2, 395, 2)\n",
    "df1 = organize_dataset(df1, 84, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_datasets(df0, df1)\n",
    "merged_df.to_csv(\"merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect NaN Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df = pd.DataFrame({\n",
    "    'ID': ['A1', 'A2', 'A3', 'A4', 'label'],\n",
    "    'Col1': [1, 2, None, 4, 1],\n",
    "    'Col2': [5, None, 7, 8, 1],\n",
    "    'Col3': [9, 10, 11, 12, 0]\n",
    "})\n",
    "\n",
    "# Column-wise check\n",
    "print(inspect_nan(df, mode=\"column\"))\n",
    "\n",
    "# Row-wise check\n",
    "print(inspect_nan(df, mode=\"row\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 1 Delta Beta Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{majority_out_path}/section_1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier in terms of every column\n",
    "def IQR(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_fence = Q3 + IQR * 1.5\n",
    "    lower_fence = Q1 - IQR * 1.5\n",
    "    return upper_fence, lower_fence\n",
    "\n",
    "\n",
    "def no_outlier(df):\n",
    "    upper_fence, lower_fence = IQR(df)\n",
    "    ddf = df[(df > lower_fence) & (df < upper_fence)]\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normal count by the count of 0 in the last row\n",
    "normal_count = int((train_df.iloc[-1, 1:] == 0).sum())\n",
    "all_beta_normalized_normal = train_df.iloc[:-1, 1 : normal_count + 1 :].T\n",
    "\n",
    "all_beta_normalized_tumor = train_df.iloc[:-1, normal_count + 1 : :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_normal = no_outlier(all_beta_normalized_normal)\n",
    "all_beta_normalized_tumor = no_outlier(all_beta_normalized_tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_avg = all_beta_normalized_normal.mean(skipna=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor = all_beta_normalized_tumor.subtract(\n",
    "    train_normal_avg, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_normalized_tumor = no_outlier(all_beta_normalized_tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tumor_mean = all_beta_normalized_tumor.mean(skipna=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta = pd.merge(\n",
    "    train_df.iloc[:-1, :1],\n",
    "    pd.DataFrame(train_tumor_mean, columns=[\"dbeta\"]),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the list of feature with dbeta being NaN\n",
    "update_nested_toml(\"preprocess.dbeta\", \"delta_beta_avg_feature_num\", delta_beta.shape[0])\n",
    "update_nested_toml(\n",
    "    \"preprocess.dbeta\",\n",
    "    \"NaN_dbeta_feature\",\n",
    "    delta_beta.loc[pd.isna(delta_beta[\"dbeta\"]), \"Unnamed: 0\"].tolist(),\n",
    ")\n",
    "delta_beta = delta_beta.dropna(axis=0)\n",
    "update_nested_toml(\"preprocess.dbeta\", \"delta_beta_avg_feature_num_remove_NaN\", delta_beta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmp = pd.read_csv(\"../prostate/champ_result/GSE269244/DMP_result_TN.csv\")\n",
    "dmp = dmp[[\"Unnamed: 0\", \"gene\", \"feature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_nested_toml(\"preprocess.dbeta\", \"dmp_before_dropna_shape_feature\", dmp.shape[0])\n",
    "\n",
    "dmp = dmp.dropna(axis=0)\n",
    "\n",
    "update_nested_toml(\"preprocess.dbeta\", \"dmp_after_dropna_shape_feature\", dmp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(delta_beta, dmp, on=\"Unnamed: 0\", how=\"left\")\n",
    "update_nested_toml(\n",
    "    \"preprocess.dbeta\", \"delta_beta_avg_feature_num_remove_NaN_join_dmp\", result.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dBeta_grouped(group):\n",
    "    idx_max = group[\"dbeta\"].abs().idxmax()\n",
    "    return group.loc[idx_max]\n",
    "\n",
    "\n",
    "dbeta_info = result.groupby(\"gene\", as_index=False).apply(\n",
    "    find_max_dBeta_grouped, include_groups=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info.columns = [\"gene\", \"ID\", \"dbeta\", \"feature\"]\n",
    "dbeta_info = dbeta_info[[\"ID\", \"gene\", \"dbeta\", \"feature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comorbidity = pd.read_csv(\n",
    "#     \"../external_result/matchgene174_single_3Y10__OR2.txt\", sep=\"\\t\", header=None\n",
    "# )\n",
    "# dbeta = dbeta[\n",
    "#     dbeta[\"gene\"].isin(comorbidity[0])\n",
    "# ]\n",
    "\n",
    "# result_max_per_gene_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info[\"dbeta\"] = dbeta_info[\"dbeta\"].apply(lambda x: round(x, 6))\n",
    "dbeta_info.to_csv(f\"{majority_out_path}/section_1/dbeta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 2 Filter Genes by Average Delta Beta Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Filtering TSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{majority_out_path}/section_2\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbeta_info = pd.read_csv(f\"{majority_out_path}/dbeta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS = dbeta_info[dbeta_info[\"feature\"].str.contains(\"TSS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS.to_csv(f\"{majority_out_path}/section_2/dbeta_TSS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "dbeta_TSS_threshold = TSS[abs(TSS[\"dbeta\"]) > threshold]\n",
    "while True:\n",
    "    dbeta_TSS_threshold = TSS[abs(TSS[\"dbeta\"]) > threshold]\n",
    "    count = dbeta_TSS_threshold.shape[0]\n",
    "    if (\n",
    "        config[\"preprocess\"][\"filtering\"][\"hyper\"][\"avg_dbeta_lower_bound\"]\n",
    "        <= count\n",
    "        <= config[\"preprocess\"][\"filtering\"][\"hyper\"][\"avg_dbeta_upper_bound\"]\n",
    "    ):\n",
    "        break\n",
    "    threshold -= 0.01\n",
    "threshold = round(threshold, 2)\n",
    "update_nested_toml(\"preprocess.filtering\", \"threshold\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_TSS_threshold.to_csv(f\"{majority_out_path}/section_2/dbeta_TSS_{threshold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.kdeplot(dbeta_TSS_threshold[\"dbeta\"])\n",
    "plt.xlabel(\"delta Beta value\")\n",
    "plt.title(\"Density plot of delta Beta value\")\n",
    "plt.savefig(f\"{majority_out_path}/section_2/dbeta_TSS_{threshold}.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(train_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = train_df.iloc[:-1, :]\n",
    "beta_df = beta_df[beta_df[\"Unnamed: 0\"].isin(dbeta_info[\"ID\"])]\n",
    "X = beta_df.iloc[:, 1:].dropna(axis=0).T\n",
    "y = train_df.iloc[-1, 1:].astype(int).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {len(y)}\")\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Principal Component 1\": X_pca[:, 0],\n",
    "        \"Principal Component 2\": X_pca[:, 1],\n",
    "        \"Principal Component 3\": X_pca[:, 2],\n",
    "        \"Class\": y,\n",
    "    }\n",
    ")\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"Principal Component 1\",\n",
    "    y=\"Principal Component 2\",\n",
    "    z=\"Principal Component 3\",\n",
    "    color=\"Class\",\n",
    "    title=\"PCA of Dataset\",\n",
    "    color_continuous_scale=\"Viridis\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{majority_out_path}/section_2/preprocess_filtering_pca.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remember to run clustering on the filtered genes first continue to the next step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 Feature Selection with ML (SFS)\n",
    "\n",
    "sequential forward selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove previous results\n",
    "\n",
    "Warning: This step is not reversible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "if os.path.exists(f\"{majority_out_path}/sfs\"):\n",
    "    shutil.rmtree(f\"{majority_out_path}/sfs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preparation(SFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "train_out_path = config[\"feature_selection\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"feature_selection\"][\"hyper\"][\"validate_out_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{train_out_path}\", exist_ok=True)\n",
    "os.makedirs(f\"{validate_out_path}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{dbeta_info_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if logs/ folder exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "from utils.train_helper import TrainHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(dbeta_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Selection(SFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(f\"{train_out_path}\")\n",
    "# validate_df = pd.read_csv(f\"{validate_out_path}\")\n",
    "# th.set_train_validate_df(train_df, validate_df)\n",
    "\n",
    "train_df = pd.read_csv(train_df_file)\n",
    "validate_df_file = config[\"feature_selection\"][\"hyper\"][\"validate_df_file\"]\n",
    "validate_df = pd.read_csv(validate_df_file)\n",
    "th.set_train_validate_df(train_df,validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "th.set_train_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{majority_out_path}/sfs\", exist_ok=True)\n",
    "\n",
    "th.select_feature_sfs(\n",
    "    out_path = f\"{majority_out_path}/sfs/selected_feature.txt\",\n",
    "    step= 4,\n",
    "    n_features_to_select=\"cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 Feature Selection with ML (RFE)\n",
    "\n",
    "recursive feature elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove previous results\n",
    "\n",
    "Warning: This step is not reversible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# if os.path.exists(f\"{majority_out_path}/section_3/rfe\"):\n",
    "#     shutil.rmtree(f\"{majority_out_path}/section_3/rfe\")\n",
    "# if os.path.exists(f\"{minority_out_path}/section_3/rfe\"):\n",
    "#     shutil.rmtree(f\"{minority_out_path}/section_3/rfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "train_out_path = config[\"feature_selection\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"feature_selection\"][\"hyper\"][\"validate_out_path\"]\n",
    "training_param_file = config[\"feature_selection\"][\"hyper\"][\"training_param_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{train_out_path}\", exist_ok=True)\n",
    "os.makedirs(f\"{validate_out_path}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "\n",
    "TSS_threshold = pd.read_csv(dbeta_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import TrainHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(TSS_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.process_norm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_df_file)\n",
    "validate_df_file = config[\"feature_selection\"][\"hyper\"][\"validate_df_file\"]\n",
    "validate_df = pd.read_csv(validate_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = inspect_nan(train_df, mode=\"column\", remove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_nan(train_df, mode=\"column\")\n",
    "inspect_nan(validate_df, mode=\"column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_train_validate_df(train_df, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import set_parameters\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import json\n",
    "\n",
    "with open(training_param_file, \"r\") as f:\n",
    "    training_param = json.load(f)\n",
    "xgb_grid = set_parameters(XGBClassifier(random_state=42), training_param[\"XGBoost\"])\n",
    "rf_grid = set_parameters(\n",
    "    RandomForestClassifier(random_state=42), training_param[\"RandomForest\"]\n",
    ")\n",
    "svm_grid = set_parameters(SVC(random_state=42, probability=True), training_param[\"SVM\"])\n",
    "dt_grid = set_parameters(\n",
    "    DecisionTreeClassifier(random_state=42), training_param[\"DecisionTree\"]\n",
    ")\n",
    "\n",
    "train_models = {\n",
    "    \"XGBoost\": xgb_grid,\n",
    "    \"RandomForest\": rf_grid,\n",
    "    \"SVM\": svm_grid,\n",
    "    \"DecisionTree\": dt_grid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "th.set_grid_estimators(train_models)\n",
    "th.set_train_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.select_feature_rfe(\n",
    "    train_out_path = train_out_path,\n",
    "    validate_out_path = validate_out_path,\n",
    "    selected_feature_path = f\"{train_out_path}/selected_feature.txt\",\n",
    "    feature_range = (1, 6, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 4 Clean Selected Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate Feature json for SimpleModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"machine_learning\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{majority_out_path}/{dbeta_info_file}\")\n",
    "th = TrainHelper(dbeta_info)\n",
    "\n",
    "folder = \"sfs\"\n",
    "features = read_selected_features(f\"{majority_out_path}/{folder}/selected_feature.txt\")\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{majority_out_path}/{folder}/selected_features.json\",\n",
    "    mode=\"min\",\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "# use this to read json\n",
    "read_selected_features_json(f\"{majority_out_path}/{folder}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Gather Selected gene list from best selection model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_train = pd.read_csv(f\"{train_out_path}/rfe.csv\")\n",
    "rfe_validate = pd.read_csv(f\"{validate_out_path}/rfe.csv\")\n",
    "fpr_tpr_train = pd.read_csv(f\"{train_out_path}/roc_curve.csv\")\n",
    "fpr_tpr_validate = pd.read_csv(f\"{validate_out_path}/roc_curve.csv\")\n",
    "rfe_j = pd.merge(rfe_train, rfe_validate, on=[\"selection_model\", \"train_model\", \"features\"], suffixes=('_train', '_validate'))\n",
    "fpr_tpr_j = pd.merge(fpr_tpr_train, fpr_tpr_validate, on=[\"selection_model\", \"train_model\", \"features\"], suffixes=('_train', '_validate'))\n",
    "J = pd.merge(rfe_j, fpr_tpr_j, on=[\"selection_model\", \"train_model\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "J[\"fpr_train\"] = J[\"fpr_train\"].apply(ast.literal_eval)\n",
    "J[\"tpr_train\"] = J[\"tpr_train\"].apply(ast.literal_eval)\n",
    "J[\"fpr_validate\"] = J[\"fpr_validate\"].apply(ast.literal_eval)\n",
    "J[\"tpr_validate\"] = J[\"tpr_validate\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.painter import plot_roc_curve, create_performance_barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J['accuracy_diff'] = J['accuracy_train'] - J['accuracy_validate']\n",
    "J['recall_diff'] = J['recall_train'] - J['recall_validate']\n",
    "J['f1_score_diff'] = J['f1_score_train'] - J['f1_score_validate']\n",
    "J['AUC_diff'] = J['AUC_train'] - J['AUC_validate']\n",
    "J['MCC_diff'] = J['MCC_train'] - J['MCC_validate']\n",
    "J['fbeta2_score_diff'] = J['fbeta2_score_train'] - J['fbeta2_score_validate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweakable width and height\n",
    "plot_roc_curve(\n",
    "    J,\n",
    "    \"ROC Curves on Training Set\",\n",
    "    f\"{train_out_path}/roc_curve.html\",\n",
    "    x_column = \"fpr_train\",\n",
    "    y_column = \"tpr_train\",\n",
    "    trace_name = [\"selection_model\", \"train_model\", \"features\"],\n",
    ")\n",
    "# tweakable width and height\n",
    "plot_roc_curve(\n",
    "    J,\n",
    "    \"ROC Curves on Testing Set\",\n",
    "    f\"{validate_out_path}/roc_curve.html\",\n",
    "    x_column = \"fpr_validate\",\n",
    "    y_column = \"tpr_validate\",\n",
    "    trace_name = [\"selection_model\", \"train_model\", \"features\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot difference\n",
    "performance_metrics = ['accuracy_diff', 'recall_diff', 'f1_score_diff', 'AUC_diff', 'MCC_diff', 'fbeta2_score_diff']\n",
    "ground_by_train_model = J.groupby('train_model')[performance_metrics].mean()\n",
    "ground_by_train_model['train_model'] = ground_by_train_model.index\n",
    "ground_by_train_model.to_csv(f\"{validate_out_path}/performance_diff_grouped_by_train_model.csv\", index=False)\n",
    "color_mapping = {\n",
    "    \"accuracy_diff\": \"blue\",\n",
    "    \"recall_diff\": \"red\",\n",
    "    \"f1_score_diff\": \"green\",\n",
    "    \"AUC_diff\": \"purple\",\n",
    "    \"MCC_diff\": \"orange\",\n",
    "    \"fbeta2_score_diff\": \"brown\",\n",
    "}\n",
    "create_performance_barchart(\n",
    "    df=ground_by_train_model,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"train_model\",\n",
    "    out_path=f\"{validate_out_path}/performance_diff_grouped_by_train_model.html\",\n",
    "    title=\"Grouped Performance Difference between Training and Testing Set\",\n",
    "    x_axis_label=\"Performance Difference (Training - Testing)\",\n",
    "    y_axis_label=\"Train Model\",\n",
    "    orientation=\"h\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J[[\"selection_model\", \"train_model\", \"features\", \"accuracy_validate\", \"recall_validate\", \"f1_score_validate\", \"AUC_validate\", \"MCC_validate\", \"fbeta2_score_validate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by train_model, for each train_model, calculate the mean of each performance metric\n",
    "performance_metrics = ['accuracy_validate', 'recall_validate',\n",
    "                       'f1_score_validate', 'AUC_validate', 'MCC_validate']\n",
    "ground_by_train_model = J.groupby('train_model')[performance_metrics].mean()\n",
    "ground_by_train_model['train_model'] = ground_by_train_model.index\n",
    "ground_by_train_model.to_csv(\n",
    "    f\"{validate_out_path}/performance_metrics_grouped_by_train_model.csv\", index=False)\n",
    "color_mapping = {\n",
    "    \"accuracy_validate\": \"blue\",\n",
    "    \"recall_validate\": \"red\",\n",
    "    \"f1_score_validate\": \"green\",\n",
    "    \"AUC_validate\": \"purple\",\n",
    "    \"MCC_validate\": \"orange\",\n",
    "}\n",
    "create_performance_barchart(\n",
    "    df=ground_by_train_model,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"train_model\",\n",
    "    out_path=f\"{validate_out_path}/performance_metrics_grouped_by_train_model.html\",\n",
    "    title=\"Grouped Performance Metrics by Train Model\",\n",
    "    x_axis_label=\"Performance\",\n",
    "    y_axis_label=\"Train Model\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "best_train_model = ground_by_train_model['MCC_validate'].idxmax()\n",
    "print(f\"Best train model: {best_train_model}\")\n",
    "ground_by_feature = J[J['train_model'] == best_train_model].groupby('features')[\n",
    "    performance_metrics].mean()\n",
    "ground_by_feature['features'] = ground_by_feature.index\n",
    "ground_by_feature.to_csv(\n",
    "    f\"{validate_out_path}/performance_metrics_grouped_by_feature.csv\", index=False)\n",
    "create_performance_barchart(\n",
    "    df=ground_by_feature,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"features\",\n",
    "    out_path=f\"{validate_out_path}/performance_metrics_grouped_by_feature.html\",\n",
    "    title=\"Grouped Performance Metrics by Feature\",\n",
    "    x_axis_label=\"Performance\",\n",
    "    y_axis_label=\"Feature\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "best_num_of_feature = ground_by_feature['MCC_validate'].idxmax()\n",
    "print(f\"Best number of feature: {best_num_of_feature}\")\n",
    "best_performance_records = J[(J['train_model'] == best_train_model) & (\n",
    "    J['features'] == best_num_of_feature)]\n",
    "best_performance_records.to_csv(\n",
    "    f\"{validate_out_path}/best_performance_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(dbeta_info_file)\n",
    "th = TrainHelper(dbeta_info)\n",
    "\n",
    "features = read_selected_features(f\"{train_out_path}/selected_feature.txt\")\n",
    "\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{validate_out_path}/selected_features.json\",\n",
    "    mode=int(best_num_of_feature),\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "# use this to read json\n",
    "read_selected_features_json(f\"{validate_out_path}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 5 Clustering Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data\n",
    "\n",
    "remember to calculate distance matrix first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.clustering_helper import hierarchical_clustering, check_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prefix = config[\"clustering_visual\"][\"hyper\"][\"result_prefix\"]\n",
    "dbeta_file = config[\"clustering_visual\"][\"hyper\"][\"dbeta_file\"]\n",
    "bp_file = config[\"clustering_visual\"][\"hyper\"][\"bp_file\"]\n",
    "cc_file = config[\"clustering_visual\"][\"hyper\"][\"cc_file\"]\n",
    "mf_file = config[\"clustering_visual\"][\"hyper\"][\"mf_file\"]\n",
    "terms_count_file = config[\"clustering_visual\"][\"hyper\"][\"terms_count_file\"]\n",
    "result_out_path = config[\"clustering_visual\"][\"hyper\"][\"result_out_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(result_out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_set = pd.read_csv(dbeta_file, index_col=0)\n",
    "distance_matrix_bp = pd.read_csv(bp_file, index_col=0)\n",
    "distance_matrix_cc = pd.read_csv(cc_file, index_col=0)\n",
    "distance_matrix_mf = pd.read_csv(mf_file, index_col=0)\n",
    "terms_count = pd.read_csv(terms_count_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 0\n",
    "distance_matrix_bp = distance_matrix_bp.fillna(0)\n",
    "distance_matrix_cc = distance_matrix_cc.fillna(0)\n",
    "distance_matrix_mf = distance_matrix_mf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex distance matrix\n",
    "index_bp = distance_matrix_bp.index\n",
    "index_cc = distance_matrix_cc.index\n",
    "index_mf = distance_matrix_mf.index\n",
    "index = index_bp.union(index_cc).union(index_mf)\n",
    "distance_matrix_bp_ = distance_matrix_bp.reindex(index=index, columns=index, fill_value=0)\n",
    "distance_matrix_cc_ = distance_matrix_cc.reindex(index=index, columns=index, fill_value=0)\n",
    "distance_matrix_mf_ = distance_matrix_mf.reindex(index=index, columns=index, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a array of distance matrix for each ontology\n",
    "distance_matrix = []\n",
    "\n",
    "distance_matrix.append(distance_matrix_bp_)\n",
    "distance_matrix.append(distance_matrix_cc_)\n",
    "distance_matrix.append(distance_matrix_mf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Weighted Sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [count for count in terms_count[\"count\"]]\n",
    "weight = weight / np.sum(weight)\n",
    "masks = np.array([~np.isnan(distance_matrix[i].values) for i in range(3)])\n",
    "\n",
    "valid_weights = np.array([weight[i] for i in range(3)])[:, None, None] * masks\n",
    "\n",
    "weight_sums = valid_weights.sum(axis=0)\n",
    "\n",
    "normalized_weights = np.divide(valid_weights, weight_sums, where=weight_sums != 0)\n",
    "weighted_sum = sum(\n",
    "    np.nan_to_num(distance_matrix[i].values) * normalized_weights[i] for i in range(3)\n",
    ")\n",
    "\n",
    "\n",
    "weighted_sum_dataframe = pd.DataFrame(weighted_sum, index=index, columns=index)\n",
    "\n",
    "weighted_sum_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_weighted = hierarchical_clustering(\n",
    "    weighted_sum_dataframe,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_weighted_sum.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_weighted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simple average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [1, 1, 1]\n",
    "masks = np.array([~np.isnan(distance_matrix[i].values) for i in range(3)])\n",
    "valid_weights = np.array([weight[i] for i in range(3)])[:, None, None] * masks\n",
    "weight_sums = valid_weights.sum(axis=0)\n",
    "normalized_weights = np.divide(valid_weights, weight_sums, where=weight_sums != 0)\n",
    "weighted_sum = sum(\n",
    "    np.nan_to_num(distance_matrix[i].values) * normalized_weights[i] for i in range(3)\n",
    ")\n",
    "simple_sum_dataframe = pd.DataFrame(weighted_sum, index=index, columns=index)\n",
    "simple_sum_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_simple = hierarchical_clustering(\n",
    "    simple_sum_dataframe,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_simple_sum.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Consensus clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bp = hierarchical_clustering(\n",
    "    distance_matrix_bp, out_path=f\"{result_out_path}/hierarchical_clustering_bp.png\"\n",
    ")\n",
    "cluster_cc = hierarchical_clustering(\n",
    "    distance_matrix_cc, out_path=f\"{result_out_path}/hierarchical_clustering_cc.png\"\n",
    ")\n",
    "cluster_mf = hierarchical_clustering(\n",
    "    distance_matrix_mf, out_path=f\"{result_out_path}/hierarchical_clustering_mf.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bp.columns = [\"gene\", \"cluster_bp\"]\n",
    "cluster_cc.columns = [\"gene\", \"cluster_cc\"]\n",
    "cluster_mf.columns = [\"gene\", \"cluster_mf\"]\n",
    "cluster_bp_cc = pd.merge(cluster_bp, cluster_cc, on=\"gene\", how=\"outer\")\n",
    "cluster_go = pd.merge(cluster_bp_cc, cluster_mf, on=\"gene\", how=\"outer\")\n",
    "cluster_go = cluster_go.fillna(-1)\n",
    "print(cluster_go.shape)\n",
    "cluster_go.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = cluster_go.shape[0]\n",
    "consensus_matrix = np.zeros((num_genes, num_genes))\n",
    "for i in range(num_genes):\n",
    "    for j in range(i, num_genes):\n",
    "        if cluster_go.iloc[i][\"cluster_bp\"] == cluster_go.iloc[j][\"cluster_bp\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "        if cluster_go.iloc[i][\"cluster_cc\"] == cluster_go.iloc[j][\"cluster_cc\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "        if cluster_go.iloc[i][\"cluster_mf\"] == cluster_go.iloc[j][\"cluster_mf\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "consensus_matrix = pd.DataFrame(\n",
    "    consensus_matrix, index=cluster_go[\"gene\"], columns=cluster_go[\"gene\"]\n",
    ")\n",
    "consensus_matrix += consensus_matrix.T\n",
    "distance_matrix_consensus = 1 - consensus_matrix / 3\n",
    "np.fill_diagonal(distance_matrix_consensus.values, 0)\n",
    "distance_matrix_consensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_consensus = hierarchical_clustering(\n",
    "    distance_matrix_consensus,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=4,\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_consensus.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_consensus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering_helper import hierarchical_clustering_compare\n",
    "\n",
    "hierarchical_clustering_compare(\n",
    "    [weighted_sum_dataframe, simple_sum_dataframe, distance_matrix_consensus],\n",
    "    [\"Weighted Average\", \"Simple Average\", \"Consensus\"],\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_compare.png\",\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info = pd.read_csv(dbeta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column gene isin weighted_sum_dataframe\n",
    "weighted_dbeta = dbeta_info[dbeta_info[\"gene\"].isin(weighted_sum_dataframe.index)]\n",
    "simple_dbeta = dbeta_info[dbeta_info[\"gene\"].isin(simple_sum_dataframe.index)]\n",
    "consensus_dbeta = dbeta_info[dbeta_info[\"gene\"].isin(distance_matrix_consensus.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_dbeta.merge(cluster_result_weighted, on=\"gene\").to_csv(\n",
    "    f\"{result_out_path}/{result_prefix}_weighted.csv\", index=False\n",
    ")\n",
    "simple_dbeta.merge(cluster_result_simple, on=\"gene\").to_csv(\n",
    "    f\"{result_out_path}/{result_prefix}_simple.csv\", index=False\n",
    ")\n",
    "consensus_dbeta.merge(cluster_result_consensus, on=\"gene\").to_csv(\n",
    "    f\"{result_out_path}/{result_prefix}_consensus.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 6 SimpleModel Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features_json\n",
    "import pandas as pd\n",
    "from utils.process_norm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_file = config[\"simple_model\"][\"hyper\"][\"dbeta_file\"]\n",
    "selected_feature_file = config[\"simple_model\"][\"hyper\"][\"selected_feature_file\"]\n",
    "train_out_path = config[\"simple_model\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"simple_model\"][\"hyper\"][\"validate_out_path\"]\n",
    "df_file1 = config[\"simple_model\"][\"hyper\"][\"df_file1\"]\n",
    "df_file2 = config[\"simple_model\"][\"hyper\"][\"df_file2\"]\n",
    "training_param_file = config[\"simple_model\"][\"hyper\"][\"training_param_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_file = read_selected_features_json(selected_feature_file)\n",
    "dbeta_file = pd.read_csv(dbeta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df1 = pd.read_csv(df_file1)\n",
    "all_df2 = pd.read_csv(df_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"../config/{TYPE}.json\") as f:\n",
    "    J = json.load(f)\n",
    "data_source = \"GSE269244\"\n",
    "# data_source = input(\"Enter the data source: \")\n",
    "\n",
    "all_df1 = organize_dataset(all_df1, J[data_source][\"normal\"], J[data_source][\"tumor\"], J[data_source][\"sample_count\"])\n",
    "\n",
    "sp80 = all_df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"../config/{TYPE}.json\") as f:\n",
    "    J = json.load(f)\n",
    "data_source = \"GDC_prostate_tissue\"\n",
    "# data_source = input(\"Enter the data source: \")\n",
    "\n",
    "all_df2 = organize_dataset(all_df2, J[data_source][\"normal\"], J[data_source][\"tumor\"], J[data_source][\"sample_count\"])\n",
    "\n",
    "sp20 = all_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp80, sp20 = split_dataset(all_df, 0.2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import set_parameters\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import json\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "with open(training_param_file, \"r\") as f:\n",
    "    training_param = json.load(f)\n",
    "\n",
    "xgb_grid = set_parameters(XGBClassifier(random_state=42), training_param[\"XGBoost\"])\n",
    "rf_grid = set_parameters(RandomForestClassifier(random_state=42), training_param[\"RandomForest\"])\n",
    "svm_grid = set_parameters(SVC(random_state=42, probability=True), training_param[\"SVM\"])\n",
    "dt_grid = set_parameters(DecisionTreeClassifier(random_state=42), training_param[\"DecisionTree\"])\n",
    "voting = VotingClassifier(\n",
    "    estimators=[(\"XGBoost\", XGBClassifier(random_state=42)), (\"RandomForest\", RandomForestClassifier(random_state=42)), (\"SVM\", SVC(random_state=42, probability=True)), (\"DecisionTree\", DecisionTreeClassifier(random_state=42))\n",
    "                ],\n",
    "    voting=\"soft\",\n",
    ")\n",
    "\n",
    "# comment out the model you don't want to use\n",
    "models = {\n",
    "    \"XGBoost\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": xgb_grid,\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": rf_grid,\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": svm_grid,\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": dt_grid,\n",
    "    },\n",
    "    \"Voting\": {\n",
    "        \"is_grid_search\": False,\n",
    "        \"model\": voting,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simple_model import SimpleModel\n",
    "\n",
    "for model_name, gene_list in selected_feature_file.items():\n",
    "    for model_name, model_config in models.items():\n",
    "        model = SimpleModel(\n",
    "            train_df=sp80,\n",
    "            test_df=sp20,\n",
    "            gene_list=gene_list,\n",
    "            dbeta_info=dbeta_file,\n",
    "        )\n",
    "        model.setup_dbeta()\n",
    "        model.setup_train_test()\n",
    "        model.setup_combinations()\n",
    "        model.train(\n",
    "            model_name,\n",
    "            model_config[\"model\"],\n",
    "            train_out_path,\n",
    "            validate_out_path,\n",
    "            model_config[\"is_grid_search\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = input(\"Enter the model name: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congratulation! You have finished the whole pipeline. <br>\n",
    "\n",
    "![title](cat.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
