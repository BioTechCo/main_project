{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. -1 Initiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.config_helper import update_nested_toml, load_config\n",
    "os.makedirs(\"logs\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE = input(\"Enter the type of the config file: \")\n",
    "TYPE = \"prostate_plan_A\"\n",
    "CONFIG_PATH = f\"../config/{TYPE}.toml\"\n",
    "config = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_1 = config[\"init\"][\"hyper\"][\"df_file_1\"]\n",
    "df_file_2 = config[\"init\"][\"hyper\"][\"df_file_2\"]\n",
    "dmp_file_1 = config[\"init\"][\"hyper\"][\"dmp_file_1\"]\n",
    "dmp_file_2 = config[\"init\"][\"hyper\"][\"dmp_file_2\"]\n",
    "majority_out_path_1 = config[\"init\"][\"hyper\"][\"majority_out_path_1\"]\n",
    "majority_out_path_2 = config[\"init\"][\"hyper\"][\"majority_out_path_2\"]\n",
    "joined_out_path = config[\"init\"][\"hyper\"][\"joined_out_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 0 process norm examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split Dataset Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.process_norm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "# cancer_type = input(\"Enter the cancer type: \")\n",
    "cancer_type = \"prostate\"\n",
    "with open(f\"../config/{cancer_type}.json\") as f:\n",
    "    J = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_source = input(\"Enter the raw all_beta_normalized file: \")\n",
    "data_source = \"GSE269244_80\"\n",
    "df = pd.read_csv(J[data_source]['file'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 96 1\n"
     ]
    }
   ],
   "source": [
    "print(J[data_source][\"normal\"],J[data_source][\"tumor\"],J[data_source][\"sample_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO complement_df feature: 695603\n",
      "INFO complement_df sample (normal, tumor): (70, 72)\n",
      "INFO ratio_df feature: 695603\n",
      "INFO ratio_df sample (normal, tumor): (24, 24)\n"
     ]
    }
   ],
   "source": [
    "dfo = organize_dataset(df, J[data_source][\"normal\"], J[data_source][\"tumor\"], J[data_source][\"sample_count\"])\n",
    "complement_df, ratio_df = split_dataset(dfo, J[data_source][\"split_test\"], J[data_source][\"random_state\"])\n",
    "complement_df.to_csv(\"all_beta_normalized_80.csv\", index=False)\n",
    "ratio_df.to_csv(\"all_beta_normalized_20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Merge Dataset Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(f\"../stomach/champ_result/gdc_stomach_GSE99553/all_beta_normalized_0.csv\")\n",
    "df1 = pd.read_csv(f\"../stomach/champ_result/gdc_stomach_GSE99553/all_beta_normalized_GSE99553.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = organize_dataset(df0, 2, 395, 2)\n",
    "df1 = organize_dataset(df1, 84, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_datasets(df0, df1)\n",
    "merged_df.to_csv(\"merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect NaN Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df = pd.DataFrame({\n",
    "    'ID': ['A1', 'A2', 'A3', 'A4', 'label'],\n",
    "    'Col1': [1, 2, None, 4, 1],\n",
    "    'Col2': [5, None, 7, 8, 1],\n",
    "    'Col3': [9, 10, 11, 12, 0]\n",
    "})\n",
    "\n",
    "# Column-wise check\n",
    "print(inspect_nan(df, mode=\"column\"))\n",
    "\n",
    "# Row-wise check\n",
    "print(inspect_nan(df, mode=\"row\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 1 Delta Beta Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dbeta_avg_helper import get_dbeta_avg, drop_dbeta_nan, get_dbeta_info\n",
    "\n",
    "os.makedirs(f\"{majority_out_path_1}/section_1\", exist_ok=True)\n",
    "os.makedirs(f\"{majority_out_path_2}/section_1\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(df_file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta = get_dbeta_avg(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dbeta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cg00000957</td>\n",
       "      <td>0.018932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cg00001349</td>\n",
       "      <td>0.040178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cg00001583</td>\n",
       "      <td>0.256413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cg00002028</td>\n",
       "      <td>0.004746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cg00002719</td>\n",
       "      <td>0.423447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373336</th>\n",
       "      <td>cg27657363</td>\n",
       "      <td>-0.005472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373337</th>\n",
       "      <td>cg27657537</td>\n",
       "      <td>0.260572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373338</th>\n",
       "      <td>cg27662611</td>\n",
       "      <td>-0.004693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373339</th>\n",
       "      <td>cg27665648</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373340</th>\n",
       "      <td>label</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0     dbeta\n",
       "0       cg00000957  0.018932\n",
       "1       cg00001349  0.040178\n",
       "2       cg00001583  0.256413\n",
       "3       cg00002028  0.004746\n",
       "4       cg00002719  0.423447\n",
       "...            ...       ...\n",
       "373336  cg27657363 -0.005472\n",
       "373337  cg27657537  0.260572\n",
       "373338  cg27662611 -0.004693\n",
       "373339  cg27665648  0.000021\n",
       "373340       label       NaN\n",
       "\n",
       "[373341 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the list of feature with dbeta being NaN\n",
    "delta_beta = drop_dbeta_nan(delta_beta, log_postfix=\"TCGA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmp = pd.read_csv(dmp_file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>gene</th>\n",
       "      <th>dbeta</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cg03630821</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>0.173038</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cg27394794</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>-0.329788</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cg07196505</td>\n",
       "      <td>A2BP1</td>\n",
       "      <td>-0.414458</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cg00336946</td>\n",
       "      <td>A2LD1</td>\n",
       "      <td>-0.258079</td>\n",
       "      <td>TSS1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cg00134295</td>\n",
       "      <td>A2M</td>\n",
       "      <td>-0.192465</td>\n",
       "      <td>TSS1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18647</th>\n",
       "      <td>cg23995459</td>\n",
       "      <td>ZYG11B</td>\n",
       "      <td>-0.164786</td>\n",
       "      <td>TSS1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18648</th>\n",
       "      <td>cg18074834</td>\n",
       "      <td>ZYX</td>\n",
       "      <td>0.037660</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18649</th>\n",
       "      <td>cg16463044</td>\n",
       "      <td>ZZEF1</td>\n",
       "      <td>-0.142714</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18650</th>\n",
       "      <td>cg22221847</td>\n",
       "      <td>ZZZ3</td>\n",
       "      <td>-0.027186</td>\n",
       "      <td>3'UTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18651</th>\n",
       "      <td>cg20009101</td>\n",
       "      <td>psiTPTE22</td>\n",
       "      <td>0.236101</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18652 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID       gene     dbeta  feature\n",
       "0      cg03630821       A1BG  0.173038     Body\n",
       "1      cg27394794       A1CF -0.329788     Body\n",
       "2      cg07196505      A2BP1 -0.414458     Body\n",
       "3      cg00336946      A2LD1 -0.258079  TSS1500\n",
       "4      cg00134295        A2M -0.192465  TSS1500\n",
       "...           ...        ...       ...      ...\n",
       "18647  cg23995459     ZYG11B -0.164786  TSS1500\n",
       "18648  cg18074834        ZYX  0.037660     Body\n",
       "18649  cg16463044      ZZEF1 -0.142714     Body\n",
       "18650  cg22221847       ZZZ3 -0.027186    3'UTR\n",
       "18651  cg20009101  psiTPTE22  0.236101     Body\n",
       "\n",
       "[18652 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbeta_info = get_dbeta_info(delta_beta, dmp, log_postfix=\"TCGA\")\n",
    "dbeta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info[\"dbeta\"] = dbeta_info[\"dbeta\"].apply(lambda x: round(x, 6))\n",
    "dbeta_info.to_csv(f\"{majority_out_path_1}/section_1/dbeta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(df_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta = get_dbeta_avg(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_beta = drop_dbeta_nan(delta_beta, log_postfix=\"GEO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmp = pd.read_csv(dmp_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>gene</th>\n",
       "      <th>dbeta</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cg22286978</td>\n",
       "      <td>A1BG</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cg03817621</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>-0.132061</td>\n",
       "      <td>5'UTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cg08390979</td>\n",
       "      <td>A2BP1</td>\n",
       "      <td>-0.203781</td>\n",
       "      <td>5'UTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cg07218357</td>\n",
       "      <td>A2LD1</td>\n",
       "      <td>-0.093872</td>\n",
       "      <td>TSS200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cg00146928</td>\n",
       "      <td>A2M</td>\n",
       "      <td>-0.081349</td>\n",
       "      <td>TSS1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>cg09704136</td>\n",
       "      <td>ZYX</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17990</th>\n",
       "      <td>cg16463044</td>\n",
       "      <td>ZZEF1</td>\n",
       "      <td>-0.095281</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>cg17033546</td>\n",
       "      <td>ZZZ3</td>\n",
       "      <td>-0.041094</td>\n",
       "      <td>5'UTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>cg20009101</td>\n",
       "      <td>psiTPTE22</td>\n",
       "      <td>0.087387</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17993</th>\n",
       "      <td>cg25020073</td>\n",
       "      <td>tAKR</td>\n",
       "      <td>0.064425</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17994 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID       gene     dbeta  feature\n",
       "0      cg22286978       A1BG  0.031693     Body\n",
       "1      cg03817621       A1CF -0.132061    5'UTR\n",
       "2      cg08390979      A2BP1 -0.203781    5'UTR\n",
       "3      cg07218357      A2LD1 -0.093872   TSS200\n",
       "4      cg00146928        A2M -0.081349  TSS1500\n",
       "...           ...        ...       ...      ...\n",
       "17989  cg09704136        ZYX  0.080000     Body\n",
       "17990  cg16463044      ZZEF1 -0.095281     Body\n",
       "17991  cg17033546       ZZZ3 -0.041094    5'UTR\n",
       "17992  cg20009101  psiTPTE22  0.087387     Body\n",
       "17993  cg25020073       tAKR  0.064425     Body\n",
       "\n",
       "[17994 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbeta_info = get_dbeta_info(delta_beta, dmp, log_postfix=\"GEO\")\n",
    "dbeta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info[\"dbeta\"] = dbeta_info[\"dbeta\"].apply(lambda x: round(x, 6))\n",
    "dbeta_info.to_csv(f\"{majority_out_path_2}/section_1/dbeta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 2 Filter Genes by Average Delta Beta Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Filtering TSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{majority_out_path_1}/section_2\", exist_ok=True)\n",
    "os.makedirs(f\"{majority_out_path_2}/section_2\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info_1 = pd.read_csv(f\"{majority_out_path_1}/section_1/dbeta.csv\")\n",
    "dbeta_info_2 = pd.read_csv(f\"{majority_out_path_2}/section_1/dbeta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS_1 = dbeta_info_1[dbeta_info_1[\"feature\"].str.contains(\"TSS\")]\n",
    "TSS_2 = dbeta_info_2[dbeta_info_2[\"feature\"].str.contains(\"TSS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSS_1.to_csv(f\"{majority_out_path_1}/section_2/dbeta_TSS.csv\", index=False)\n",
    "TSS_2.to_csv(f\"{majority_out_path_2}/section_2/dbeta_TSS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dbeta_avg_helper import detect_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_TSS_threshold_1, threshold_1 = detect_threshold(TSS_1, config=config, log_postfix=\"_TCGA\")\n",
    "dbeta_TSS_threshold_1.to_csv(f\"{majority_out_path_1}/section_2/dbeta_TSS_{threshold_1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_TSS_threshold_2, threshold_2 = detect_threshold(TSS_2, config=config, log_postfix=\"_GEO\")\n",
    "dbeta_TSS_threshold_2.to_csv(f\"{majority_out_path_2}/section_2/dbeta_TSS_{threshold_2}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dbeta_avg_helper import dbeta_graph, pca_graph\n",
    "\n",
    "dbeta_graph(dbeta_TSS_threshold_1, f\"{majority_out_path_1}/section_2/dbeta_TSS_{threshold_1}.png\")\n",
    "dbeta_graph(dbeta_TSS_threshold_2, f\"{majority_out_path_2}/section_2/dbeta_TSS_{threshold_2}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(df_file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_graph(dbeta_info_1, df_1, f\"{majority_out_path_1}/section_2/pca.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(df_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_graph(dbeta_info_2, df_2, f\"{majority_out_path_2}/section_2/pca.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 join dbeta_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{joined_out_path}/section_2\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info_1 = pd.read_csv(f\"{majority_out_path_1}/section_2/dbeta_TSS_{threshold_1}.csv\")\n",
    "dbeta_info_2 = pd.read_csv(f\"{majority_out_path_2}/section_2/dbeta_TSS_{threshold_2}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(dbeta_info_1, dbeta_info_2, on=\"gene\", how=\"inner\", suffixes=('_TCGA', '_GEO'))\n",
    "merged_df = merged_df[[\"gene\"] + [col for col in merged_df.columns if col != \"gene\"]]\n",
    "merged_df.to_csv(f\"{joined_out_path}/section_2/dbeta_TSS_threshold_joined.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remember to run clustering on the filtered genes first continue to the next step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 Feature Selection with ML (SFS)\n",
    "\n",
    "sequential forward selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove previous results\n",
    "\n",
    "Warning: This step is not reversible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# if os.path.exists(f\"{majority_out_path}/section_3/sfs\"):\n",
    "#     shutil.rmtree(f\"{majority_out_path}/section_3/sfs\")\n",
    "# if os.path.exists(f\"{minority_out_path}/section_3/sfs\"):\n",
    "#     shutil.rmtree(f\"{minority_out_path}/section_3/sfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preparation(SFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "train_out_path = config[\"feature_selection\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"feature_selection\"][\"hyper\"][\"validate_out_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{train_out_path}\", exist_ok=True)\n",
    "os.makedirs(f\"{validate_out_path}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(f\"{majority_out_path}/{dbeta_info_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if logs/ folder exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "from utils.train_helper import TrainHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(dbeta_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Selection(SFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{majority_out_path}/{majority_df_path}\")\n",
    "validate_df = pd.read_csv(f\"{minority_out_path}/{minority_df_path}\")\n",
    "th.set_train_validate_df(train_df, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=10,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "th.set_train_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{majority_out_path}/sfs\", exist_ok=True)\n",
    "\n",
    "th.select_feature_sfs(\n",
    "    out_path = f\"{majority_out_path}/sfs/selected_feature.txt\",\n",
    "    step= 4,\n",
    "    n_features_to_select=\"cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 3 Feature Selection with ML (RFE)\n",
    "\n",
    "recursive feature elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove previous results\n",
    "\n",
    "Warning: This step is not reversible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# if os.path.exists(f\"{majority_out_path}/section_3/rfe\"):\n",
    "#     shutil.rmtree(f\"{majority_out_path}/section_3/rfe\")\n",
    "# if os.path.exists(f\"{minority_out_path}/section_3/rfe\"):\n",
    "#     shutil.rmtree(f\"{minority_out_path}/section_3/rfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "train_out_path = config[\"feature_selection\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"feature_selection\"][\"hyper\"][\"validate_out_path\"]\n",
    "training_param_file = config[\"feature_selection\"][\"hyper\"][\"training_param_file\"]\n",
    "os.makedirs(f\"{train_out_path}\", exist_ok=True)\n",
    "os.makedirs(f\"{validate_out_path}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "TSS_threshold = pd.read_csv(dbeta_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import TrainHelper\n",
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(TSS_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.process_norm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(df_file_1)\n",
    "validate_df_file = config[\"feature_selection\"][\"hyper\"][\"validate_df_file\"]\n",
    "validate_df = pd.read_csv(validate_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = inspect_nan(train_df, mode=\"column\", remove=True)\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_train_validate_df(train_df, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.train_helper import set_parameters\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(training_param_file, \"r\") as f:\n",
    "#     training_param = json.load(f)\n",
    "# xgb_grid = set_parameters(XGBClassifier(random_state=42), training_param[\"XGBoost\"])\n",
    "# rf_grid = set_parameters(\n",
    "#     RandomForestClassifier(random_state=42), training_param[\"RandomForest\"]\n",
    "# )\n",
    "# svm_grid = set_parameters(SVC(random_state=42, probability=True), training_param[\"SVM\"])\n",
    "# dt_grid = set_parameters(\n",
    "#     DecisionTreeClassifier(random_state=42), training_param[\"DecisionTree\"]\n",
    "# )\n",
    "\n",
    "# train_models = {\n",
    "#     \"XGBoost\": xgb_grid,\n",
    "#     \"RandomForest\": rf_grid,\n",
    "#     \"SVM\": svm_grid,\n",
    "#     \"DecisionTree\": dt_grid,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=50,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=50,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "# th.set_grid_estimators(train_models)\n",
    "th.set_train_validate(ID=\"ID_TCGA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.select_feature_rfe(\n",
    "    train_out_path = train_out_path,\n",
    "    validate_out_path = validate_out_path,\n",
    "    selected_feature_path = f\"{train_out_path}/selected_feature.txt\",\n",
    "    feature_range = \"cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Preparation (second round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "train_out_path = config[\"feature_selection_2\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"feature_selection_2\"][\"hyper\"][\"validate_out_path\"]\n",
    "training_param_file = config[\"feature_selection_2\"][\"hyper\"][\"training_param_file\"]\n",
    "os.makedirs(f\"{train_out_path}\", exist_ok=True)\n",
    "os.makedirs(f\"{validate_out_path}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "TSS_threshold = pd.read_csv(dbeta_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import TrainHelper\n",
    "# note that there is setup_dbeta in TrainHelper to further cut down the feature size\n",
    "th = TrainHelper(TSS_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Selection (second round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(df_file_2)\n",
    "validate_df_file = config[\"feature_selection_2\"][\"hyper\"][\"validate_df_file\"]\n",
    "validate_df = pd.read_csv(validate_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_train_validate_df(train_df, validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_models = {\n",
    "    \"SVM\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=50,\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=50,\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.set_selection_models(selection_models)\n",
    "# th.set_grid_estimators(train_models)\n",
    "th.set_train_validate(ID=\"ID_GEO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.select_feature_rfe(\n",
    "    train_out_path = train_out_path,\n",
    "    validate_out_path = validate_out_path,\n",
    "    selected_feature_path = f\"{train_out_path}/selected_feature.txt\",\n",
    "    feature_range = \"cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 4 Clean Selected Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate Feature json for SimpleModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(dbeta_info_file)\n",
    "th = TrainHelper(dbeta_info)\n",
    "method = \"rfe\"\n",
    "features = read_selected_features(f\"{majority_out_path_1}/section_3/{method}/selected_feature.txt\")\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{majority_out_path_1}/section_3/{method}/selected_features.json\",\n",
    "    mode=\"min\",\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "read_selected_features_json(f\"{majority_out_path_1}/section_3/{method}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"feature_selection_2\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(dbeta_info_file)\n",
    "th = TrainHelper(dbeta_info)\n",
    "method = \"rfe\"\n",
    "features = read_selected_features(f\"{majority_out_path_2}/section_3/{method}/selected_feature.txt\")\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{majority_out_path_2}/section_3/{method}/selected_features.json\",\n",
    "    mode=\"min\",\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "# use this to read json\n",
    "read_selected_features_json(f\"{majority_out_path_2}/section_3/{method}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Gather Selected gene list from best selection model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_train = pd.read_csv(f\"{train_out_path}/rfe.csv\")\n",
    "rfe_validate = pd.read_csv(f\"{validate_out_path}/rfe.csv\")\n",
    "fpr_tpr_train = pd.read_csv(f\"{train_out_path}/roc_curve.csv\")\n",
    "fpr_tpr_validate = pd.read_csv(f\"{validate_out_path}/roc_curve.csv\")\n",
    "rfe_j = pd.merge(rfe_train, rfe_validate, on=[\"selection_model\", \"train_model\", \"features\"], suffixes=('_train', '_validate'))\n",
    "fpr_tpr_j = pd.merge(fpr_tpr_train, fpr_tpr_validate, on=[\"selection_model\", \"train_model\", \"features\"], suffixes=('_train', '_validate'))\n",
    "J = pd.merge(rfe_j, fpr_tpr_j, on=[\"selection_model\", \"train_model\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "J[\"fpr_train\"] = J[\"fpr_train\"].apply(ast.literal_eval)\n",
    "J[\"tpr_train\"] = J[\"tpr_train\"].apply(ast.literal_eval)\n",
    "J[\"fpr_validate\"] = J[\"fpr_validate\"].apply(ast.literal_eval)\n",
    "J[\"tpr_validate\"] = J[\"tpr_validate\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.painter import plot_roc_curve, create_performance_barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J['accuracy_diff'] = J['accuracy_train'] - J['accuracy_validate']\n",
    "J['recall_diff'] = J['recall_train'] - J['recall_validate']\n",
    "J['f1_score_diff'] = J['f1_score_train'] - J['f1_score_validate']\n",
    "J['AUC_diff'] = J['AUC_train'] - J['AUC_validate']\n",
    "J['MCC_diff'] = J['MCC_train'] - J['MCC_validate']\n",
    "J['fbeta2_score_diff'] = J['fbeta2_score_train'] - J['fbeta2_score_validate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweakable width and height\n",
    "plot_roc_curve(\n",
    "    J,\n",
    "    \"ROC Curves on Training Set\",\n",
    "    f\"{train_out_path}/roc_curve.html\",\n",
    "    x_column = \"fpr_train\",\n",
    "    y_column = \"tpr_train\",\n",
    "    trace_name = [\"selection_model\", \"train_model\", \"features\"],\n",
    ")\n",
    "# tweakable width and height\n",
    "plot_roc_curve(\n",
    "    J,\n",
    "    \"ROC Curves on Testing Set\",\n",
    "    f\"{validate_out_path}/roc_curve.html\",\n",
    "    x_column = \"fpr_validate\",\n",
    "    y_column = \"tpr_validate\",\n",
    "    trace_name = [\"selection_model\", \"train_model\", \"features\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot difference\n",
    "performance_metrics = ['accuracy_diff', 'recall_diff', 'f1_score_diff', 'AUC_diff', 'MCC_diff', 'fbeta2_score_diff']\n",
    "ground_by_train_model = J.groupby('train_model')[performance_metrics].mean()\n",
    "ground_by_train_model['train_model'] = ground_by_train_model.index\n",
    "ground_by_train_model.to_csv(f\"{validate_out_path}/performance_diff_grouped_by_train_model.csv\", index=False)\n",
    "color_mapping = {\n",
    "    \"accuracy_diff\": \"blue\",\n",
    "    \"recall_diff\": \"red\",\n",
    "    \"f1_score_diff\": \"green\",\n",
    "    \"AUC_diff\": \"purple\",\n",
    "    \"MCC_diff\": \"orange\",\n",
    "    \"fbeta2_score_diff\": \"brown\",\n",
    "}\n",
    "create_performance_barchart(\n",
    "    df=ground_by_train_model,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"train_model\",\n",
    "    out_path=f\"{validate_out_path}/performance_diff_grouped_by_train_model.html\",\n",
    "    title=\"Grouped Performance Difference between Training and Testing Set\",\n",
    "    x_axis_label=\"Performance Difference (Training - Testing)\",\n",
    "    y_axis_label=\"Train Model\",\n",
    "    orientation=\"h\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J[[\"selection_model\", \"train_model\", \"features\", \"accuracy_validate\", \"recall_validate\", \"f1_score_validate\", \"AUC_validate\", \"MCC_validate\", \"fbeta2_score_validate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by train_model, for each train_model, calculate the mean of each performance metric\n",
    "performance_metrics = ['accuracy_validate', 'recall_validate',\n",
    "                       'f1_score_validate', 'AUC_validate', 'MCC_validate']\n",
    "ground_by_train_model = J.groupby('train_model')[performance_metrics].mean()\n",
    "ground_by_train_model['train_model'] = ground_by_train_model.index\n",
    "ground_by_train_model.to_csv(\n",
    "    f\"{validate_out_path}/performance_metrics_grouped_by_train_model.csv\", index=False)\n",
    "color_mapping = {\n",
    "    \"accuracy_validate\": \"blue\",\n",
    "    \"recall_validate\": \"red\",\n",
    "    \"f1_score_validate\": \"green\",\n",
    "    \"AUC_validate\": \"purple\",\n",
    "    \"MCC_validate\": \"orange\",\n",
    "}\n",
    "create_performance_barchart(\n",
    "    df=ground_by_train_model,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"train_model\",\n",
    "    out_path=f\"{validate_out_path}/performance_metrics_grouped_by_train_model.html\",\n",
    "    title=\"Grouped Performance Metrics by Train Model\",\n",
    "    x_axis_label=\"Performance\",\n",
    "    y_axis_label=\"Train Model\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "best_train_model = ground_by_train_model['MCC_validate'].idxmax()\n",
    "print(f\"Best train model: {best_train_model}\")\n",
    "ground_by_feature = J[J['train_model'] == best_train_model].groupby('features')[\n",
    "    performance_metrics].mean()\n",
    "ground_by_feature['features'] = ground_by_feature.index\n",
    "ground_by_feature.to_csv(\n",
    "    f\"{validate_out_path}/performance_metrics_grouped_by_feature.csv\", index=False)\n",
    "create_performance_barchart(\n",
    "    df=ground_by_feature,\n",
    "    color_mapping=color_mapping,\n",
    "    metric=\"features\",\n",
    "    out_path=f\"{validate_out_path}/performance_metrics_grouped_by_feature.html\",\n",
    "    title=\"Grouped Performance Metrics by Feature\",\n",
    "    x_axis_label=\"Performance\",\n",
    "    y_axis_label=\"Feature\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "best_num_of_feature = ground_by_feature['MCC_validate'].idxmax()\n",
    "print(f\"Best number of feature: {best_num_of_feature}\")\n",
    "best_performance_records = J[(J['train_model'] == best_train_model) & (\n",
    "    J['features'] == best_num_of_feature)]\n",
    "best_performance_records.to_csv(\n",
    "    f\"{validate_out_path}/best_performance_records.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features, read_selected_features_json, TrainHelper\n",
    "config = load_config(CONFIG_PATH)\n",
    "dbeta_info_file = config[\"feature_selection\"][\"hyper\"][\"dbeta_info_file\"]\n",
    "dbeta_info = pd.read_csv(dbeta_info_file)\n",
    "th = TrainHelper(dbeta_info)\n",
    "\n",
    "features = read_selected_features(f\"{train_out_path}/selected_feature.txt\")\n",
    "\n",
    "th.generate_selected_features(\n",
    "    features,\n",
    "    f\"{validate_out_path}/selected_features.json\",\n",
    "    mode=int(best_num_of_feature),\n",
    "    out_format=\"json\",\n",
    ")\n",
    "\n",
    "# use this to read json\n",
    "read_selected_features_json(f\"{validate_out_path}/selected_features.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 5 Clustering Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data\n",
    "\n",
    "remember to calculate distance matrix first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.clustering_helper import hierarchical_clustering, check_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prefix = config[\"clustering_visual\"][\"hyper\"][\"result_prefix\"]\n",
    "dbeta_file = config[\"clustering_visual\"][\"hyper\"][\"dbeta_file\"]\n",
    "bp_file = config[\"clustering_visual\"][\"hyper\"][\"bp_file\"]\n",
    "cc_file = config[\"clustering_visual\"][\"hyper\"][\"cc_file\"]\n",
    "mf_file = config[\"clustering_visual\"][\"hyper\"][\"mf_file\"]\n",
    "terms_count_file = config[\"clustering_visual\"][\"hyper\"][\"terms_count_file\"]\n",
    "result_out_path = config[\"clustering_visual\"][\"hyper\"][\"result_out_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(result_out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_set = pd.read_csv(dbeta_file, index_col=0)\n",
    "distance_matrix_bp = pd.read_csv(bp_file, index_col=0)\n",
    "distance_matrix_cc = pd.read_csv(cc_file, index_col=0)\n",
    "distance_matrix_mf = pd.read_csv(mf_file, index_col=0)\n",
    "terms_count = pd.read_csv(terms_count_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 0\n",
    "distance_matrix_bp = distance_matrix_bp.fillna(0)\n",
    "distance_matrix_cc = distance_matrix_cc.fillna(0)\n",
    "distance_matrix_mf = distance_matrix_mf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex distance matrix\n",
    "index_bp = distance_matrix_bp.index\n",
    "index_cc = distance_matrix_cc.index\n",
    "index_mf = distance_matrix_mf.index\n",
    "index = index_bp.union(index_cc).union(index_mf)\n",
    "distance_matrix_bp_ = distance_matrix_bp.reindex(index=index, columns=index, fill_value=0)\n",
    "distance_matrix_cc_ = distance_matrix_cc.reindex(index=index, columns=index, fill_value=0)\n",
    "distance_matrix_mf_ = distance_matrix_mf.reindex(index=index, columns=index, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a array of distance matrix for each ontology\n",
    "distance_matrix = []\n",
    "\n",
    "distance_matrix.append(distance_matrix_bp_)\n",
    "distance_matrix.append(distance_matrix_cc_)\n",
    "distance_matrix.append(distance_matrix_mf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Weighted Sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [count for count in terms_count[\"count\"]]\n",
    "weight = weight / np.sum(weight)\n",
    "masks = np.array([~np.isnan(distance_matrix[i].values) for i in range(3)])\n",
    "\n",
    "valid_weights = np.array([weight[i] for i in range(3)])[:, None, None] * masks\n",
    "\n",
    "weight_sums = valid_weights.sum(axis=0)\n",
    "\n",
    "normalized_weights = np.divide(valid_weights, weight_sums, where=weight_sums != 0)\n",
    "weighted_sum = sum(\n",
    "    np.nan_to_num(distance_matrix[i].values) * normalized_weights[i] for i in range(3)\n",
    ")\n",
    "\n",
    "\n",
    "weighted_sum_dataframe = pd.DataFrame(weighted_sum, index=index, columns=index)\n",
    "\n",
    "weighted_sum_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_weighted = hierarchical_clustering(\n",
    "    weighted_sum_dataframe,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_weighted_sum.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_weighted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simple average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [1, 1, 1]\n",
    "masks = np.array([~np.isnan(distance_matrix[i].values) for i in range(3)])\n",
    "valid_weights = np.array([weight[i] for i in range(3)])[:, None, None] * masks\n",
    "weight_sums = valid_weights.sum(axis=0)\n",
    "normalized_weights = np.divide(valid_weights, weight_sums, where=weight_sums != 0)\n",
    "weighted_sum = sum(\n",
    "    np.nan_to_num(distance_matrix[i].values) * normalized_weights[i] for i in range(3)\n",
    ")\n",
    "simple_sum_dataframe = pd.DataFrame(weighted_sum, index=index, columns=index)\n",
    "simple_sum_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_simple = hierarchical_clustering(\n",
    "    simple_sum_dataframe,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_simple_sum.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Consensus clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bp = hierarchical_clustering(\n",
    "    distance_matrix_bp, out_path=f\"{result_out_path}/hierarchical_clustering_bp.png\"\n",
    ")\n",
    "cluster_cc = hierarchical_clustering(\n",
    "    distance_matrix_cc, out_path=f\"{result_out_path}/hierarchical_clustering_cc.png\"\n",
    ")\n",
    "cluster_mf = hierarchical_clustering(\n",
    "    distance_matrix_mf, out_path=f\"{result_out_path}/hierarchical_clustering_mf.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_bp.columns = [\"gene\", \"cluster_bp\"]\n",
    "cluster_cc.columns = [\"gene\", \"cluster_cc\"]\n",
    "cluster_mf.columns = [\"gene\", \"cluster_mf\"]\n",
    "cluster_bp_cc = pd.merge(cluster_bp, cluster_cc, on=\"gene\", how=\"outer\")\n",
    "cluster_go = pd.merge(cluster_bp_cc, cluster_mf, on=\"gene\", how=\"outer\")\n",
    "cluster_go = cluster_go.fillna(-1)\n",
    "print(cluster_go.shape)\n",
    "cluster_go.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = cluster_go.shape[0]\n",
    "consensus_matrix = np.zeros((num_genes, num_genes))\n",
    "for i in range(num_genes):\n",
    "    for j in range(i, num_genes):\n",
    "        if cluster_go.iloc[i][\"cluster_bp\"] == cluster_go.iloc[j][\"cluster_bp\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "        if cluster_go.iloc[i][\"cluster_cc\"] == cluster_go.iloc[j][\"cluster_cc\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "        if cluster_go.iloc[i][\"cluster_mf\"] == cluster_go.iloc[j][\"cluster_mf\"]:\n",
    "            consensus_matrix[i][j] += 1\n",
    "\n",
    "consensus_matrix = pd.DataFrame(\n",
    "    consensus_matrix, index=cluster_go[\"gene\"], columns=cluster_go[\"gene\"]\n",
    ")\n",
    "consensus_matrix += consensus_matrix.T\n",
    "distance_matrix_consensus = 1 - consensus_matrix / 3\n",
    "np.fill_diagonal(distance_matrix_consensus.values, 0)\n",
    "distance_matrix_consensus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_consensus = hierarchical_clustering(\n",
    "    distance_matrix_consensus,\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    "    cluster_number=3,\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_consensus.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result_consensus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering_helper import hierarchical_clustering_compare\n",
    "\n",
    "hierarchical_clustering_compare(\n",
    "    [weighted_sum_dataframe, simple_sum_dataframe, distance_matrix_consensus],\n",
    "    [\"Weighted Average\", \"Simple Average\", \"Consensus\"],\n",
    "    out_path=f\"{result_out_path}/hierarchical_clustering_compare.png\",\n",
    "    range_min=2,\n",
    "    range_max=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_info = pd.read_csv(dbeta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column gene isin weighted_sum_dataframe\n",
    "weighted_dbeta = dbeta_info[dbeta_info[\"gene\"].isin(weighted_sum_dataframe.index)]\n",
    "simple_dbeta = dbeta_info[dbeta_info[\"gene\"].isin(simple_sum_dataframe.index)]\n",
    "consensus_dbeta = dbeta_info[dbeta_info[\"gene\"].isin(distance_matrix_consensus.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_dbeta.merge(cluster_result_weighted, on=\"gene\").to_csv(\n",
    "    f\"{result_out_path}/{result_prefix}_weighted.csv\", index=False\n",
    ")\n",
    "simple_dbeta.merge(cluster_result_simple, on=\"gene\").to_csv(\n",
    "    f\"{result_out_path}/{result_prefix}_simple.csv\", index=False\n",
    ")\n",
    "consensus_dbeta.merge(cluster_result_consensus, on=\"gene\").to_csv(\n",
    "    f\"{result_out_path}/{result_prefix}_consensus.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec. 6 SimpleModel Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import read_selected_features_json\n",
    "import pandas as pd\n",
    "from utils.process_norm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbeta_file = config[\"simple_model\"][\"hyper\"][\"dbeta_file\"]\n",
    "selected_feature_file = config[\"simple_model\"][\"hyper\"][\"selected_feature_file\"]\n",
    "selected_feature_file_2 = config[\"simple_model\"][\"hyper\"][\"selected_feature_file_2\"]\n",
    "train_out_path = config[\"simple_model\"][\"hyper\"][\"train_out_path\"]\n",
    "validate_out_path = config[\"simple_model\"][\"hyper\"][\"validate_out_path\"]\n",
    "df_train_file = config[\"simple_model\"][\"hyper\"][\"df_train_file\"]\n",
    "df_test_file = config[\"simple_model\"][\"hyper\"][\"df_test_file\"]\n",
    "df_train_file_2 = config[\"simple_model\"][\"hyper\"][\"df_train_file_2\"]\n",
    "df_test_file_2 = config[\"simple_model\"][\"hyper\"][\"df_test_file_2\"]\n",
    "training_param_file = config[\"simple_model\"][\"hyper\"][\"training_param_file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature = read_selected_features_json(selected_feature_file)\n",
    "selected_feature_2 = read_selected_features_json(selected_feature_file_2)\n",
    "dbeta_file = pd.read_csv(dbeta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(df_train_file)\n",
    "test = pd.read_csv(df_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = inspect_nan(train, mode=\"column\", remove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = pd.read_csv(df_train_file_2)\n",
    "test_2 = pd.read_csv(df_test_file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merge_datasets(train, train_2)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = merge_datasets(test, test_2)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_helper import set_parameters\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import json\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "with open(training_param_file, \"r\") as f:\n",
    "    training_param = json.load(f)\n",
    "\n",
    "xgb_grid = set_parameters(XGBClassifier(random_state=42), training_param[\"XGBoost\"])\n",
    "rf_grid = set_parameters(RandomForestClassifier(random_state=42), training_param[\"RandomForest\"])\n",
    "svm_grid = set_parameters(SVC(random_state=42, probability=True), training_param[\"SVM\"])\n",
    "dt_grid = set_parameters(DecisionTreeClassifier(random_state=42), training_param[\"DecisionTree\"])\n",
    "voting = VotingClassifier(\n",
    "    estimators=[(\"XGBoost\", XGBClassifier(random_state=42)), (\"RandomForest\", RandomForestClassifier(random_state=42)), (\"SVM\", SVC(random_state=42, probability=True)), (\"DecisionTree\", DecisionTreeClassifier(random_state=42))\n",
    "                ],\n",
    "    voting=\"soft\",\n",
    ")\n",
    "\n",
    "# comment out the model you don't want to use\n",
    "models = {\n",
    "    \"XGBoost\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": xgb_grid,\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": rf_grid,\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": svm_grid,\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"is_grid_search\": True,\n",
    "        \"model\": dt_grid,\n",
    "    },\n",
    "    \"Voting\": {\n",
    "        \"is_grid_search\": False,\n",
    "        \"model\": voting,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simple_model import SimpleModel\n",
    "\n",
    "for model_name, gene_list in selected_feature.items():\n",
    "    for model_name, model_config in models.items():\n",
    "        model = SimpleModel(\n",
    "            train_df=train,\n",
    "            test_df=test,\n",
    "            gene_list=gene_list,\n",
    "            dbeta_info=dbeta_file,\n",
    "        )\n",
    "        model.setup_dbeta()\n",
    "        model.setup_train_test(\"ID_TCGA\")\n",
    "        model.setup_combinations()\n",
    "        model.train(\n",
    "            model_name,\n",
    "            model_config[\"model\"],\n",
    "            train_out_path,\n",
    "            validate_out_path,\n",
    "            model_config[\"is_grid_search\"],\n",
    "            \"ID_TCGA\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.simple_model import SimpleModel\n",
    "\n",
    "for model_name, gene_list in selected_feature_2.items():\n",
    "    for model_name, model_config in models.items():\n",
    "        model = SimpleModel(\n",
    "            train_df=train,\n",
    "            test_df=test,\n",
    "            gene_list=gene_list,\n",
    "            dbeta_info=dbeta_file,\n",
    "        )\n",
    "        model.setup_dbeta()\n",
    "        model.setup_train_test(\"ID_GEO\")\n",
    "        model.setup_combinations()\n",
    "        model.train(\n",
    "            model_name,\n",
    "            model_config[\"model\"],\n",
    "            train_out_path,\n",
    "            validate_out_path,\n",
    "            model_config[\"is_grid_search\"],\n",
    "            \"ID_GEO\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congratulation! You have finished the whole pipeline🎉. <br>\n",
    "\n",
    "![title](cat.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
